{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1337f7f",
   "metadata": {},
   "source": [
    "# 01 â€“ Data Quality Assessment\n",
    "\n",
    "This notebook analyses the raw credit application dataset for data quality issues across the following dimensions:\n",
    "\n",
    "- Completeness\n",
    "- Consistency\n",
    "- Validity\n",
    "- Accuracy\n",
    "\n",
    "All issues are quantified (counts and percentages) and mapped to governance implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6e826",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Structural Inspection\n",
    "- Load raw JSON\n",
    "- Inspect nested structure\n",
    "- Examine column names and data types\n",
    "\n",
    "## 2. Completeness Analysis\n",
    "- Missing values per column\n",
    "- Incomplete nested objects\n",
    "- % affected records\n",
    "\n",
    "## 3. Consistency & Type Validation\n",
    "- Data type mismatches\n",
    "- Inconsistent categorical encoding (e.g., gender formats)\n",
    "- Date format inconsistencies\n",
    "\n",
    "## 4. Validity Checks\n",
    "- Impossible values (e.g., negative income, negative credit history)\n",
    "- Logical inconsistencies (e.g., interest rate assigned when rejected)\n",
    "\n",
    "## 5. Duplicate Record Detection\n",
    "\n",
    "## 6. Remediation Strategy\n",
    "- Cleaning logic\n",
    "- Standardisation decisions\n",
    "- Governance implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc23179",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587f624",
   "metadata": {},
   "source": [
    "## Setup & Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2290625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/anaconda3/lib/python3.13/site-packages (4.16.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=2.6.1 in /opt/anaconda3/lib/python3.13/site-packages (from pymongo) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "!pip install pymongo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c73cd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: novacred. Dropped existing collection for fresh ingestion.\n",
      "\n",
      "--- Ingestion Audit Summary ---\n",
      "Standard Records Inserted: 500\n",
      "Duplicate IDs Rescued:    2\n",
      "Total Audit Baseline:      502 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "\n",
    "# Establish connection to the local MongoDB instance\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['novacred']\n",
    "collection = db['credit_applications']\n",
    "\n",
    "# Reset the collection to ensure an idempotent and clean baseline for the audit\n",
    "collection.drop()\n",
    "print(f\"Connected to: {db.name}. Dropped existing collection for fresh ingestion.\")\n",
    "\n",
    "# Determine the canonical data path based on the current working directory\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == 'notebooks':\n",
    "    data_path = cwd.parent / 'data' / 'raw_credit_applications.json'\n",
    "else:\n",
    "    data_path = cwd / 'data' / 'raw_credit_applications.json'\n",
    "\n",
    "# Validate the existence of the source file before proceeding\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"CRITICAL: Could not locate data file at: {data_path}\")\n",
    "\n",
    "# Load the raw application data from the JSON file\n",
    "with data_path.open('r') as file:\n",
    "    raw_data = json.load(file)\n",
    "\n",
    "# Execute the fault-tolerant ingestion loop\n",
    "successful_inserts = 0\n",
    "rescued_duplicates = 0\n",
    "\n",
    "for doc in raw_data:\n",
    "    try:\n",
    "        # Attempt to insert the document into the collection\n",
    "        collection.insert_one(doc)\n",
    "        successful_inserts += 1\n",
    "    except DuplicateKeyError:\n",
    "        # Append a suffix to the primary key to rescue colliding records for audit purposes\n",
    "        doc['_id'] = f\"{doc['_id']}_duplicate\"\n",
    "        collection.insert_one(doc)\n",
    "        rescued_duplicates += 1\n",
    "\n",
    "# Output the final ingestion metrics to verify the audit baseline\n",
    "print(\"\\n--- Ingestion Audit Summary ---\")\n",
    "print(f\"Standard Records Inserted: {successful_inserts}\")\n",
    "print(f\"Duplicate IDs Rescued:    {rescued_duplicates}\")\n",
    "print(f\"Total Audit Baseline:      {collection.count_documents({})} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9186f7e",
   "metadata": {},
   "source": [
    "**Ingestion Audit Note:**\n",
    " \n",
    "MongoDB automatically rejected 2 records (app_042, app_001) during import due to E11000 duplicate key errors on the _id field. To prevent data loss and ensure 100% auditability, these records were rescued by appending a _duplicate suffix to their _id. Baseline record count established at 502 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd16562",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76598b1",
   "metadata": {},
   "source": [
    "## Quick Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67c1113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'app_200',\n",
      " 'applicant_info': {'date_of_birth': '2001-03-09',\n",
      "                    'email': 'jerry.smith17@hotmail.com',\n",
      "                    'full_name': 'Jerry Smith',\n",
      "                    'gender': 'Male',\n",
      "                    'ip_address': '192.168.48.155',\n",
      "                    'ssn': '596-64-4340',\n",
      "                    'zip_code': '10036'},\n",
      " 'decision': {'loan_approved': False,\n",
      "              'rejection_reason': 'algorithm_risk_score'},\n",
      " 'financials': {'annual_income': 73000,\n",
      "                'credit_history_months': 23,\n",
      "                'debt_to_income': 0.2,\n",
      "                'savings_balance': 31212},\n",
      " 'processing_timestamp': '2024-01-15T00:00:00Z',\n",
      " 'spending_behavior': [{'amount': 480, 'category': 'Shopping'},\n",
      "                       {'amount': 790, 'category': 'Rent'},\n",
      "                       {'amount': 247, 'category': 'Alcohol'}]}\n"
     ]
    }
   ],
   "source": [
    "# View a sample document from the collection\n",
    "sample = collection.find_one()\n",
    "pprint(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a0bef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089353a2",
   "metadata": {},
   "source": [
    "## Audit Query 1: Find Duplicates | Data Quality Dimension: Uniqueness\n",
    "\n",
    "Potential issue: The same person is appearing multiple times in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27d48fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Identifiers (Completeness Issues) ---\n",
      " Found 5 records without an SSN.\n",
      " Names: ['Margaret Williams', 'Carolyn Martin', 'Larry Williams', 'Stephanie Nguyen', 'Brandon Moore']\n",
      " IDs:   ['app_075', 'app_120', 'app_268', 'app_001_duplicate', 'app_165']\n",
      "\n",
      "--- Duplicate Identifiers (Uniqueness Issues) ---\n",
      "Found 3 duplicate SSNs:\n",
      " SSN: 780-24-9300 - Count: 2\n",
      "   Names: ['Susan Martinez', 'Gary Wilson']\n",
      "   IDs:   ['app_088', 'app_016']\n",
      "\n",
      " SSN: 652-70-5530 - Count: 2\n",
      "   Names: ['Joseph Lopez', 'Joseph Lopez']\n",
      "   IDs:   ['app_042', 'app_042_duplicate']\n",
      "\n",
      " SSN: 937-72-8731 - Count: 2\n",
      "   Names: ['Sandra Smith', 'Samuel Hill']\n",
      "   IDs:   ['app_101', 'app_234']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the aggregation pipeline to find duplicate SSNs - each person should appear only once!\n",
    "pipeline_duplicates = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$applicant_info.ssn\",\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            \"names\": {\"$push\": \"$applicant_info.full_name\"},\n",
    "            \"original_ids\": {\"$push\": \"$_id\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$match\": {\n",
    "            \"count\": {\"$gt\": 1}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute the aggregation pipeline against the credit_applications collection\n",
    "duplicates = list(collection.aggregate(pipeline_duplicates))\n",
    "\n",
    "# Separate the results into missing identifiers and duplicate identifiers\n",
    "missing_ssn = [dup for dup in duplicates if dup['_id'] is None]\n",
    "actual_duplicates = [dup for dup in duplicates if dup['_id'] is not None]\n",
    "\n",
    "# Output findings for missing identifiers to identify completeness failures\n",
    "if missing_ssn:\n",
    "    print(f\"--- Missing Identifiers (Completeness Issues) ---\")\n",
    "    for item in missing_ssn:\n",
    "        print(f\" Found {item['count']} records without an SSN.\")\n",
    "        print(f\" Names: {item['names']}\")\n",
    "        print(f\" IDs:   {item['original_ids']}\\n\")\n",
    "\n",
    "# Output findings for duplicate identifiers to identify system errors or fraud\n",
    "if actual_duplicates:\n",
    "    print(f\"--- Duplicate Identifiers (Uniqueness Issues) ---\")\n",
    "    print(f\"Found {len(actual_duplicates)} duplicate SSNs:\")\n",
    "    for dup in actual_duplicates:\n",
    "        print(f\" SSN: {dup['_id']} - Count: {dup['count']}\")\n",
    "        print(f\"   Names: {dup['names']}\")\n",
    "        print(f\"   IDs:   {dup['original_ids']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8417a4",
   "metadata": {},
   "source": [
    "**Uniqueness Audit: Initial Findings**\n",
    "\n",
    "The uniqueness audit of the 502-record baseline identifies 11 violating documents (2.19% of the dataset) across four distinct failure groups. These findings include five applicants with missing SSN identifiers (e.g., Margaret Williams, Carolyn Martin) and six records involved in identity collisions where three SSNs are shared by multiple entries. Specifically, these collisions range from exact system duplicates (Joseph Lopez) to conflicting identities sharing a single identifier (Susan Martinez and Gary Wilson), suggesting both system integration errors and potential fraud attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b87d85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Uniqueness Health Metric ---\n",
      "Total Records: 502\n",
      "Records with Violations: 11\n",
      "Uniqueness Score: 97.81%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Uniqueness KPI to quantify data quality health\n",
    "total_records = collection.count_documents({})\n",
    "violation_count = sum(item['count'] for item in duplicates)\n",
    "unique_score = ((total_records - violation_count) / total_records) * 100\n",
    "\n",
    "# Output the Uniqueness Health Metric for the audit report\n",
    "print(f\"--- Uniqueness Health Metric ---\")\n",
    "print(f\"Total Records: {total_records}\")\n",
    "print(f\"Records with Violations: {violation_count}\")\n",
    "print(f\"Uniqueness Score: {unique_score:.2f}%\\n\")\n",
    "\n",
    "# Execute the remediation to quarantine failing records as required for deduplication\n",
    "# Map the specific IDs from the aggregation results into a flat list\n",
    "failing_ids = []\n",
    "for result in duplicates:\n",
    "    # Note: Ensure your pipeline above includes \"record_ids\": {\"$push\": \"$_id\"} to support this\n",
    "    failing_ids.extend(result.get('record_ids', []))\n",
    "\n",
    "# Move the identified violations to a dedicated quarantine collection\n",
    "if failing_ids:\n",
    "    quarantine_collection = db['quarantine_uniqueness']\n",
    "    \n",
    "    # Retrieve the full documents from the main collection\n",
    "    records_to_quarantine = list(collection.find({\"_id\": {\"$in\": failing_ids}}))\n",
    "    \n",
    "    # Insert the records into the quarantine collection and remove them from the active set\n",
    "    quarantine_collection.insert_many(records_to_quarantine)\n",
    "    collection.delete_many({\"_id\": {\"$in\": failing_ids}})\n",
    "    \n",
    "    print(f\"--- Remediation Executed ---\")\n",
    "    print(f\"Successfully moved {len(failing_ids)} records to 'quarantine_uniqueness'.\")\n",
    "    print(f\"Cleaned Baseline for further auditing: {collection.count_documents({})} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10692ba",
   "metadata": {},
   "source": [
    "**Uniqueness KPI and Remediation**\n",
    "\n",
    "To quantify the impact on data quality, a Uniqueness Health Metric of 97.81% is established, representing the ratio of compliant records to the total audit baseline. From a governance perspective, these quality gaps violate AI Act \"High-Quality Data\" standards (Art. 10) and compromise the \"Right to Explanation\" for affected individuals. Following the mandate to investigate and deduplicate, all 11 records are moved to the quarantine_uniqueness collection. This remediation ensures a clean, unique baseline of 491 documents for subsequent bias and fairness audits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c340327a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
