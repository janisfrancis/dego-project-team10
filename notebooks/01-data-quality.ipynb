{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1337f7f",
   "metadata": {},
   "source": [
    "# 01 â€“ Data Quality Assessment\n",
    "\n",
    "This notebook analyses the raw credit application dataset for data quality issues across the following dimensions:\n",
    "\n",
    "- Completeness\n",
    "- Consistency\n",
    "- Validity\n",
    "- Accuracy\n",
    "\n",
    "All issues are quantified (counts and percentages) and mapped to governance implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6e826",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Structural Inspection\n",
    "- Load raw JSON\n",
    "- Inspect nested structure\n",
    "- Examine column names and data types\n",
    "\n",
    "## 2. Completeness Analysis\n",
    "- Missing values per column\n",
    "- Incomplete nested objects\n",
    "- % affected records\n",
    "\n",
    "## 3. Consistency & Type Validation\n",
    "- Data type mismatches\n",
    "- Inconsistent categorical encoding (e.g., gender formats)\n",
    "- Date format inconsistencies\n",
    "\n",
    "## 4. Validity Checks\n",
    "- Impossible values (e.g., negative income, negative credit history)\n",
    "- Logical inconsistencies (e.g., interest rate assigned when rejected)\n",
    "\n",
    "## 5. Duplicate Record Detection\n",
    "\n",
    "## 6. Remediation Strategy\n",
    "- Cleaning logic\n",
    "- Standardisation decisions\n",
    "- Governance implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc23179",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587f624",
   "metadata": {},
   "source": [
    "## Setup & Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2290625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/anaconda3/lib/python3.13/site-packages (4.16.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=2.6.1 in /opt/anaconda3/lib/python3.13/site-packages (from pymongo) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "!pip install pymongo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c73cd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: novacred. Dropped existing collection for fresh ingestion.\n",
      "\n",
      "--- Ingestion Audit Summary ---\n",
      "Standard Records Inserted: 500\n",
      "Duplicate IDs Rescued:    2\n",
      "Total Audit Baseline:      502 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "\n",
    "# Establish connection to the local MongoDB instance\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['novacred']\n",
    "collection = db['credit_applications']\n",
    "\n",
    "# Reset the collection to ensure an idempotent and clean baseline for the audit\n",
    "collection.drop()\n",
    "print(f\"Connected to: {db.name}. Dropped existing collection for fresh ingestion.\")\n",
    "\n",
    "# Determine the canonical data path based on the current working directory\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == 'notebooks':\n",
    "    data_path = cwd.parent / 'data' / 'raw_credit_applications.json'\n",
    "else:\n",
    "    data_path = cwd / 'data' / 'raw_credit_applications.json'\n",
    "\n",
    "# Validate the existence of the source file before proceeding\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"CRITICAL: Could not locate data file at: {data_path}\")\n",
    "\n",
    "# Load the raw application data from the JSON file\n",
    "with data_path.open('r') as file:\n",
    "    raw_data = json.load(file)\n",
    "\n",
    "# Execute the fault-tolerant ingestion loop\n",
    "successful_inserts = 0\n",
    "rescued_duplicates = 0\n",
    "\n",
    "for doc in raw_data:\n",
    "    try:\n",
    "        # Attempt to insert the document into the collection\n",
    "        collection.insert_one(doc)\n",
    "        successful_inserts += 1\n",
    "    except DuplicateKeyError:\n",
    "        # Append a suffix to the primary key to rescue colliding records for audit purposes\n",
    "        doc['_id'] = f\"{doc['_id']}_duplicate\"\n",
    "        collection.insert_one(doc)\n",
    "        rescued_duplicates += 1\n",
    "\n",
    "# Output the final ingestion metrics to verify the audit baseline\n",
    "print(\"\\n--- Ingestion Audit Summary ---\")\n",
    "print(f\"Standard Records Inserted: {successful_inserts}\")\n",
    "print(f\"Duplicate IDs Rescued:    {rescued_duplicates}\")\n",
    "print(f\"Total Audit Baseline:      {collection.count_documents({})} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9186f7e",
   "metadata": {},
   "source": [
    "**Ingestion Audit Note:**\n",
    " \n",
    "MongoDB automatically rejected 2 records (app_042, app_001) during import due to E11000 duplicate key errors on the _id field. To prevent data loss and ensure 100% auditability, these records were rescued by appending a _duplicate suffix to their _id. Baseline record count established at 502 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd16562",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76598b1",
   "metadata": {},
   "source": [
    "## Quick Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67c1113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'app_200',\n",
      " 'applicant_info': {'date_of_birth': '2001-03-09',\n",
      "                    'email': 'jerry.smith17@hotmail.com',\n",
      "                    'full_name': 'Jerry Smith',\n",
      "                    'gender': 'Male',\n",
      "                    'ip_address': '192.168.48.155',\n",
      "                    'ssn': '596-64-4340',\n",
      "                    'zip_code': '10036'},\n",
      " 'decision': {'loan_approved': False,\n",
      "              'rejection_reason': 'algorithm_risk_score'},\n",
      " 'financials': {'annual_income': 73000,\n",
      "                'credit_history_months': 23,\n",
      "                'debt_to_income': 0.2,\n",
      "                'savings_balance': 31212},\n",
      " 'processing_timestamp': '2024-01-15T00:00:00Z',\n",
      " 'spending_behavior': [{'amount': 480, 'category': 'Shopping'},\n",
      "                       {'amount': 790, 'category': 'Rent'},\n",
      "                       {'amount': 247, 'category': 'Alcohol'}]}\n"
     ]
    }
   ],
   "source": [
    "# View a sample document from the collection\n",
    "sample = collection.find_one()\n",
    "pprint(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a0bef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089353a2",
   "metadata": {},
   "source": [
    "## Data Quality Dimension 1: Uniqueness\n",
    "\n",
    "The **Uniqueness Audit** identified **11 violations** across the 502-record dataset, establishing an initial health score of **97.81%** and a clean baseline of **491 records**. Findings included **two technical duplicates** for **Joseph Lopez** and **Stephanie Nguyen** flagged as \"**RESUBMISSION**\" and \"**DUPLICATE_ENTRY_ERROR**\", **four identity collisions** involving shared SSNs (e.g., Martinez/Wilson), and **five completeness failures** where SSNs were missing. All **11 records** were quarantined to satisfy **AI Act Art. 10** requirements for data accuracy and uniqueness, ensuring no redundant or ambiguous entries compromise the subsequent fairness audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27d48fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Identifiers (Completeness Issues) ---\n",
      " Found 5 records without an SSN:\n",
      "  - Margaret Williams (ID: app_075)\n",
      "  - Carolyn Martin (ID: app_120)\n",
      "  - Larry Williams (ID: app_268)\n",
      "  - Stephanie Nguyen (ID: app_001_duplicate)\n",
      "  - Brandon Moore (ID: app_165)\n",
      "\n",
      "--- Duplicate Identifiers (Uniqueness Issues) ---\n",
      "Found 3 duplicate SSNs (Identity Collisions):\n",
      " SSN: 780-24-9300 - Count: 2\n",
      "  - Susan Martinez (ID: app_088)\n",
      "  - Gary Wilson (ID: app_016)\n",
      "\n",
      " SSN: 652-70-5530 - Count: 2\n",
      "  - Joseph Lopez (ID: app_042)\n",
      "  - Joseph Lopez (ID: app_042_duplicate)\n",
      "\n",
      " SSN: 937-72-8731 - Count: 2\n",
      "  - Sandra Smith (ID: app_101)\n",
      "  - Samuel Hill (ID: app_234)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the aggregation pipeline to find duplicate SSNs - each person should appear only once!\n",
    "# This query specifically addresses the Uniqueness and Completeness dimensions for the 2026 Audit.\n",
    "pipeline_duplicates = [\n",
    "    {\n",
    "        # Group by the SSN identifier to detect identity collisions and missing values\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$applicant_info.ssn\",\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            # Map names and IDs into a records array to maintain a granular audit trail\n",
    "            \"records\": {\n",
    "                \"$push\": {\n",
    "                    \"name\": \"$applicant_info.full_name\",\n",
    "                    \"id\": \"$_id\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # Filter for groups that appear more than once to isolate potential data quality violations\n",
    "        \"$match\": {\n",
    "            \"count\": {\"$gt\": 1}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # Sort by frequency to prioritize the investigation of high-risk identifier collisions\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute the aggregation pipeline against the credit_applications collection\n",
    "duplicates = list(collection.aggregate(pipeline_duplicates))\n",
    "\n",
    "# Segregate results into Missing Identifiers and Duplicate Identifiers for focused governance reporting\n",
    "missing_ssn = [dup for dup in duplicates if dup['_id'] is None]\n",
    "actual_duplicates = [dup for dup in duplicates if dup['_id'] is not None]\n",
    "\n",
    "# Output findings for missing identifiers to address the Completeness requirement of the AI Act\n",
    "if missing_ssn:\n",
    "    print(\"--- Missing Identifiers (Completeness Issues) ---\")\n",
    "    for item in missing_ssn:\n",
    "        print(f\" Found {item['count']} records without an SSN:\")\n",
    "        for record in item['records']:\n",
    "            # List each specific record with its ID for technical validation\n",
    "            print(f\"  - {record['name']} (ID: {record['id']})\")\n",
    "        print()\n",
    "\n",
    "# Output findings for duplicate identifiers to address the Uniqueness requirement for high-risk AI data\n",
    "if actual_duplicates:\n",
    "    print(\"--- Duplicate Identifiers (Uniqueness Issues) ---\")\n",
    "    print(f\"Found {len(actual_duplicates)} duplicate SSNs (Identity Collisions):\")\n",
    "    for dup in actual_duplicates:\n",
    "        print(f\" SSN: {dup['_id']} - Count: {dup['count']}\")\n",
    "        for record in dup['records']:\n",
    "            # Document individual applicants sharing a single unique identifier\n",
    "            print(f\"  - {record['name']} (ID: {record['id']})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8417a4",
   "metadata": {},
   "source": [
    "The uniqueness audit of the **502-record baseline** identifies **11 violating documents** (2.19% of the dataset) across **four distinct failure groups**. These findings include **five applicants with missing SSN identifiers**, representing a critical **completeness failure**, and **six records** involved in **identity collisions** where **three SSNs** are shared by multiple entries. These collisions range from **exact system duplicates** (Joseph Lopez) to **conflicting identities** (Susan Martinez and Gary Wilson), suggesting both **system integration errors** and **potential fraud attempts** that violate **AI Act Art. 10** high-quality data standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b15e745c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Name-Based Collision Discovery (26 groups identified) ---\n",
      "\n",
      "Name: Susan Flores | Total Records: 3\n",
      "  - Record ID: app_448 | SSN: 383-48-9078\n",
      "  - Record ID: app_073 | SSN: 470-84-5617\n",
      "  - Record ID: app_226 | SSN: 817-96-6416\n",
      "--------------------------------------------------\n",
      "Name: Amy Flores | Total Records: 2\n",
      "  - Record ID: app_212 | SSN: 654-72-8456\n",
      "  - Record ID: app_146 | SSN: 577-59-1479\n",
      "--------------------------------------------------\n",
      "Name: Rachel King | Total Records: 2\n",
      "  - Record ID: app_193 | SSN: 852-24-1787\n",
      "  - Record ID: app_418 | SSN: 107-92-5280\n",
      "--------------------------------------------------\n",
      "Name: Shirley Davis | Total Records: 2\n",
      "  - Record ID: app_148 | SSN: 384-17-7019\n",
      "  - Record ID: app_219 | SSN: 994-53-6088\n",
      "--------------------------------------------------\n",
      "Name: James Rivera | Total Records: 2\n",
      "  - Record ID: app_465 | SSN: 853-96-1952\n",
      "  - Record ID: app_498 | SSN: 942-34-6834\n",
      "--------------------------------------------------\n",
      "Name: Jerry Nguyen | Total Records: 2\n",
      "  - Record ID: app_447 | SSN: 799-19-9143\n",
      "  - Record ID: app_231 | SSN: 821-93-7079\n",
      "--------------------------------------------------\n",
      "Name: Dorothy Perez | Total Records: 2\n",
      "  - Record ID: app_118 | SSN: 132-21-8550\n",
      "  - Record ID: app_426 | SSN: 402-41-6717\n",
      "--------------------------------------------------\n",
      "Name: Janet Johnson | Total Records: 2\n",
      "  - Record ID: app_131 | SSN: 605-97-2230\n",
      "  - Record ID: app_287 | SSN: 205-59-5076\n",
      "--------------------------------------------------\n",
      "Name: Stephanie Nguyen | Total Records: 2\n",
      "  - Record ID: app_001 | SSN: 427-90-1892\n",
      "  - Record ID: app_001_duplicate | SSN: MISSING\n",
      "--------------------------------------------------\n",
      "Name: James Martin | Total Records: 2\n",
      "  - Record ID: app_208 | SSN: 155-19-6776\n",
      "  - Record ID: app_302 | SSN: 594-58-9666\n",
      "--------------------------------------------------\n",
      "Name: Brandon Moore | Total Records: 2\n",
      "  - Record ID: app_134 | SSN: 714-64-1550\n",
      "  - Record ID: app_165 | SSN: MISSING\n",
      "--------------------------------------------------\n",
      "Name: George Garcia | Total Records: 2\n",
      "  - Record ID: app_107 | SSN: 565-34-4484\n",
      "  - Record ID: app_139 | SSN: 864-96-8449\n",
      "--------------------------------------------------\n",
      "Name: Susan Young | Total Records: 2\n",
      "  - Record ID: app_288 | SSN: 159-19-1765\n",
      "  - Record ID: app_321 | SSN: 395-66-6220\n",
      "--------------------------------------------------\n",
      "Name: Larry Williams | Total Records: 2\n",
      "  - Record ID: app_034 | SSN: 142-17-2501\n",
      "  - Record ID: app_268 | SSN: MISSING\n",
      "--------------------------------------------------\n",
      "Name: Joseph Lopez | Total Records: 2\n",
      "  - Record ID: app_042 | SSN: 652-70-5530\n",
      "  - Record ID: app_042_duplicate | SSN: 652-70-5530\n",
      "--------------------------------------------------\n",
      "Name: Mark Campbell | Total Records: 2\n",
      "  - Record ID: app_195 | SSN: 730-25-4224\n",
      "  - Record ID: app_110 | SSN: 223-83-4934\n",
      "--------------------------------------------------\n",
      "Name: Katherine Lopez | Total Records: 2\n",
      "  - Record ID: app_271 | SSN: 696-78-1129\n",
      "  - Record ID: app_047 | SSN: 241-14-6672\n",
      "--------------------------------------------------\n",
      "Name: Catherine Campbell | Total Records: 2\n",
      "  - Record ID: app_366 | SSN: 352-74-1939\n",
      "  - Record ID: app_087 | SSN: 499-29-5726\n",
      "--------------------------------------------------\n",
      "Name: Patrick Martinez | Total Records: 2\n",
      "  - Record ID: app_018 | SSN: 935-32-9047\n",
      "  - Record ID: app_468 | SSN: 996-86-6099\n",
      "--------------------------------------------------\n",
      "Name: Stephanie Allen | Total Records: 2\n",
      "  - Record ID: app_069 | SSN: 916-13-8007\n",
      "  - Record ID: app_257 | SSN: 509-17-2576\n",
      "--------------------------------------------------\n",
      "Name: Emma Clark | Total Records: 2\n",
      "  - Record ID: app_007 | SSN: 191-40-8079\n",
      "  - Record ID: app_463 | SSN: 976-47-3536\n",
      "--------------------------------------------------\n",
      "Name: George Clark | Total Records: 2\n",
      "  - Record ID: app_225 | SSN: 745-55-9936\n",
      "  - Record ID: app_185 | SSN: 804-59-4925\n",
      "--------------------------------------------------\n",
      "Name: Amanda Brown | Total Records: 2\n",
      "  - Record ID: app_050 | SSN: 695-46-5371\n",
      "  - Record ID: app_197 | SSN: 885-43-6142\n",
      "--------------------------------------------------\n",
      "Name: Brian Brown | Total Records: 2\n",
      "  - Record ID: app_169 | SSN: 566-85-7180\n",
      "  - Record ID: app_164 | SSN: 586-29-4288\n",
      "--------------------------------------------------\n",
      "Name: Heather Davis | Total Records: 2\n",
      "  - Record ID: app_367 | SSN: 618-61-5297\n",
      "  - Record ID: app_013 | SSN: 941-17-4706\n",
      "--------------------------------------------------\n",
      "Name: Anna White | Total Records: 2\n",
      "  - Record ID: app_377 | SSN: 617-17-3415\n",
      "  - Record ID: app_306 | SSN: 757-27-8131\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pipeline to find name collisions and their associated SSNs for secondary discovery\n",
    "pipeline_name_discovery = [\n",
    "    {\n",
    "        # Group by full name to identify homonym groups or missing-link duplicates\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$applicant_info.full_name\",\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            \"record_details\": {\n",
    "                \"$push\": {\n",
    "                    \"id\": \"$_id\",\n",
    "                    \"ssn\": \"$applicant_info.ssn\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # Filter for names appearing more than once to detect potential duplicates\n",
    "        \"$match\": {\"count\": {\"$gt\": 1}}\n",
    "    },\n",
    "    {\n",
    "        # Sort by frequency to prioritize the investigation of common name groups\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute discovery against the current audit collection\n",
    "name_collisions = list(collection.aggregate(pipeline_name_discovery))\n",
    "\n",
    "print(f\"--- Name-Based Collision Discovery ({len(name_collisions)} groups identified) ---\\n\")\n",
    "for group in name_collisions:\n",
    "    print(f\"Name: {group['_id']} | Total Records: {group['count']}\")\n",
    "    for record in group['record_details']:\n",
    "        ssn_value = record.get('ssn', 'MISSING')\n",
    "        print(f\"  - Record ID: {record['id']} | SSN: {ssn_value}\")\n",
    "    print(\"-\" * 50) # Visual separator for cleaner reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3afe2",
   "metadata": {},
   "source": [
    "The secondary discovery phase identified **26 groups** where names appear multiple times, totaling 53 records. Cross-referencing these with unique identifiers reveals that **23 groups** are **unique individuals** sharing common names (**homonyms**), such as Susan Flores, who must remain in the dataset. Only one group is a **confirmed system duplicate** (Joseph Lopez), while two others (Stephanie Nguyen, Brandon Moore) involve records with **missing identifiers**. These results confirm that **SSN-based auditing** is the only reliable deduplication method to avoid \"**Homonym Bias**\" and comply with **AI Act Art. 10** standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18587748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global Metadata Inspection (11 Records) ---\n",
      "\n",
      "ID: app_001 | Name: Stephanie Nguyen | SSN: 427-90-1892\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_001_duplicate | Name: Stephanie Nguyen | SSN: MISSING\n",
      "  -> [METADATA FOUND in 'notes']: DUPLICATE_ENTRY_ERROR\n",
      "--------------------------------------------------\n",
      "ID: app_016 | Name: Gary Wilson | SSN: 780-24-9300\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_042 | Name: Joseph Lopez | SSN: 652-70-5530\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_042_duplicate | Name: Joseph Lopez | SSN: 652-70-5530\n",
      "  -> [METADATA FOUND in 'notes']: RESUBMISSION\n",
      "--------------------------------------------------\n",
      "ID: app_075 | Name: Margaret Williams | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_088 | Name: Susan Martinez | SSN: 780-24-9300\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_101 | Name: Sandra Smith | SSN: 937-72-8731\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_120 | Name: Carolyn Martin | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_134 | Name: Brandon Moore | SSN: 714-64-1550\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_165 | Name: Brandon Moore | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_234 | Name: Samuel Hill | SSN: 937-72-8731\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_268 | Name: Larry Williams | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive list of all 11 records identified in the Uniqueness/Completeness audit\n",
    "all_violating_ids = [\n",
    "    \"app_042\", \"app_042_duplicate\", # Joseph Lopez (Confirmed Duplicate)\n",
    "    \"app_001\", \"app_001_duplicate\", # Stephanie Nguyen (Incomplete Link)\n",
    "    \"app_088\", \"app_016\",           # Martinez/Wilson (SSN Collision)\n",
    "    \"app_101\", \"app_234\",           # Smith/Hill (SSN Collision)\n",
    "    \"app_075\", \"app_120\", \"app_268\",# Williams/Martin/Williams (Missing IDs)\n",
    "    \"app_134\", \"app_165\"            # Brandon Moore (Incomplete Link)\n",
    "]\n",
    "\n",
    "# Retrieve the full documents for all flagged identifiers\n",
    "audit_details = list(collection.find({\"_id\": {\"$in\": all_violating_ids}}))\n",
    "\n",
    "print(f\"--- Global Metadata Inspection (11 Records) ---\\n\")\n",
    "for doc in audit_details:\n",
    "    name = doc.get('applicant_info', {}).get('full_name', 'Unknown')\n",
    "    ssn = doc.get('applicant_info', {}).get('ssn', 'MISSING')\n",
    "    \n",
    "    print(f\"ID: {doc['_id']} | Name: {name} | SSN: {ssn}\")\n",
    "    \n",
    "    # Dynamically scan for any field that might contain 'resubmission' or 'note'\n",
    "    found_metadata = False\n",
    "    for key, value in doc.items():\n",
    "        # Check top-level strings and nested dictionaries for 'note' or 'resubmit'\n",
    "        if any(term in str(key).lower() or term in str(value).lower() \n",
    "               for term in ['note', 'resubmit', 'audit', 'comment', 'flag']):\n",
    "            print(f\"  -> [METADATA FOUND in '{key}']: {value}\")\n",
    "            found_metadata = True\n",
    "            \n",
    "    if not found_metadata:\n",
    "        print(\"  -> No audit notes or system flags detected.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3bd0ad",
   "metadata": {},
   "source": [
    "The global metadata inspection confirms that two records contain explicit audit flags: **app_042_duplicate** (Joseph Lopez) is marked as a \"**RESUBMISSION**\" and **app_001_duplicate** (Stephanie Nguyen) as a \"**DUPLICATE_ENTRY_ERROR**\". These notes provide evidence of **system integration failures** for these specific cases. However, the remaining **nine records**, including the **identity collisions** (Martinez/Wilson and Smith/Hill) and **missing SSNs** (Williams, Martin, Moore), lack any explanatory metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87d85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Uniqueness Health Metric (Historical Baseline) ---\n",
      "Original Baseline Total: 502\n",
      "Total Records with Violations: 11\n",
      "Uniqueness Score: 97.81%\n"
     ]
    }
   ],
   "source": [
    "# Definitive list of the 11 violating records identified for the 2026 Audit\n",
    "target_quarantine_ids = [\n",
    "    \"app_042\", \"app_001_duplicate\", \"app_016\", \"app_088\", \n",
    "    \"app_101\", \"app_234\", \"app_075\", \"app_120\", \n",
    "    \"app_268\", \"app_134\", \"app_165\"\n",
    "]\n",
    "\n",
    "# Reconstruct the 502-record baseline count by checking both collections\n",
    "# This ensures the KPI remains accurate even after remediation\n",
    "active_count = collection.count_documents({})\n",
    "quarantined_count = db.quarantine_uniqueness.count_documents({\"_id\": {\"$in\": target_quarantine_ids}})\n",
    "baseline_total = active_count + quarantined_count\n",
    "\n",
    "# Calculate metrics based on the historical baseline to quantify data health\n",
    "violation_count = len(target_quarantine_ids)\n",
    "unique_score = ((baseline_total - violation_count) / baseline_total) * 100\n",
    "\n",
    "# Output the Uniqueness Health Metric for the audit report\n",
    "print(f\"--- Uniqueness Health Metric (Historical Baseline) ---\")\n",
    "print(f\"Original Baseline Total: {baseline_total}\")\n",
    "print(f\"Total Records with Violations: {violation_count}\")\n",
    "print(f\"Uniqueness Score: {unique_score:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52026f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Uniqueness Quarantine Verification (11 Records) ---\n",
      "ID                   | Name                 | Status/Note\n",
      "------------------------------------------------------------\n",
      "app_001_duplicate    | Stephanie Nguyen     | DUPLICATE_ENTRY_ERROR\n",
      "app_016              | Gary Wilson          | No specific flag (SSN Collision/Missing)\n",
      "app_042              | Joseph Lopez         | No specific flag (SSN Collision/Missing)\n",
      "app_075              | Margaret Williams    | No specific flag (SSN Collision/Missing)\n",
      "app_088              | Susan Martinez       | No specific flag (SSN Collision/Missing)\n",
      "app_101              | Sandra Smith         | No specific flag (SSN Collision/Missing)\n",
      "app_120              | Carolyn Martin       | No specific flag (SSN Collision/Missing)\n",
      "app_134              | Brandon Moore        | No specific flag (SSN Collision/Missing)\n",
      "app_165              | Brandon Moore        | No specific flag (SSN Collision/Missing)\n",
      "app_234              | Samuel Hill          | No specific flag (SSN Collision/Missing)\n",
      "app_268              | Larry Williams       | No specific flag (SSN Collision/Missing)\n",
      "\n",
      "Audit Status: Phase 1 (Uniqueness) Verified.\n",
      "Clean Baseline: 491 records remain in the audit pool.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all documents held in the quarantine collection for verification\n",
    "# This ensures the 11 identified violations are securely segregated from the active baseline\n",
    "quarantined_audit_trail = list(db.quarantine_uniqueness.find({}))\n",
    "\n",
    "# Output the Verification Table for the Audit Report to document the exclusion rationale\n",
    "print(f\"--- Uniqueness Quarantine Verification ({len(quarantined_audit_trail)} Records) ---\")\n",
    "print(f\"{'ID':<20} | {'Name':<20} | {'Status/Note'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for record in quarantined_audit_trail:\n",
    "    # Access the applicant name and relevant audit flags for reporting transparency\n",
    "    name = record.get('applicant_info', {}).get('full_name', 'Unknown')\n",
    "    # Capture the specific 'notes' field to prove the technical reason for quarantine\n",
    "    note = record.get('notes', 'No specific flag (SSN Collision/Missing)')\n",
    "    \n",
    "    print(f\"{record['_id']:<20} | {name:<20} | {note}\")\n",
    "\n",
    "# Final validation of the remaining active records to confirm readiness for the Consistency phase\n",
    "# The goal is to verify the transition from a 502-record set to a 491-record clean baseline\n",
    "print(f\"\\nAudit Status: Phase 1 (Uniqueness) Verified.\")\n",
    "print(f\"Clean Baseline: {collection.count_documents({})} records remain in the audit pool.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c340327a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea252a",
   "metadata": {},
   "source": [
    "## Data Quality Dimension 2: Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc157b0b",
   "metadata": {},
   "source": [
    "The **Categorical Consistency Remediation** successfully resolved encoding fragmentation within the gender field of the 491-record baseline. The audit identified **109 records** using **non-standard abbreviations**: **57 labeled as \"F\"** and **52 as \"M\"** which were mapped to the required \"Female\" and \"Male\" taxonomy. This consolidation increased the Female cohort to **248** and the Male cohort to **243**, ensuring the dataset is statistically robust for the subsequent AI Act Art. 10 fairness testing without losing any valid records from the clean baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea715e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender value distribution:\n",
      "Expected: 2 distinct values (Male, Female)\n",
      "Actual: 4 distinct values\n",
      "\n",
      "  'Female': 191 records\n",
      "  'Male': 191 records\n",
      "  'F': 57 records\n",
      "  'M': 52 records\n"
     ]
    }
   ],
   "source": [
    "# How many different gender values exist?\n",
    "pipeline_gender_consistency = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$applicant_info.gender\",\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "gender_values = list(collection.aggregate(pipeline_gender_consistency))\n",
    "\n",
    "print(\"Gender value distribution:\")\n",
    "print(\"Expected: 2 distinct values (Male, Female)\")\n",
    "print(f\"Actual: {len(gender_values)} distinct values\")\n",
    "print()\n",
    "for gv in gender_values:\n",
    "    print(f\"  '{gv['_id']}': {gv['count']} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb9c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Categorical Remediation Executed ---\n",
      "Standardized 'M' -> 'Male': 0 records updated.\n",
      "Standardized 'F' -> 'Female': 0 records updated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute in-place database updates to resolve 109 categorical encoding fragmentation errors\n",
    "# Standardize gender values to align with the required taxonomy for AI Act Art. 10 fairness testing\n",
    "\n",
    "# Map 52 abbreviated 'M' encodings to the standard 'Male' category\n",
    "result_m = collection.update_many(\n",
    "    {\"applicant_info.gender\": \"M\"},\n",
    "    {\"$set\": {\"applicant_info.gender\": \"Male\"}}\n",
    ")\n",
    "\n",
    "# Map 57 abbreviated 'F' encodings to the standard 'Female' category\n",
    "result_f = collection.update_many(\n",
    "    {\"applicant_info.gender\": \"F\"},\n",
    "    {\"$set\": {\"applicant_info.gender\": \"Female\"}}\n",
    ")\n",
    "\n",
    "print(f\"--- Categorical Remediation Executed ---\")\n",
    "print(f\"Standardized 'M' -> 'Male': {result_m.modified_count} records updated.\")\n",
    "print(f\"Standardized 'F' -> 'Female': {result_f.modified_count} records updated.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0c21dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Consistency Verification: applicant_info.gender ---\n",
      "Expected standard values: ['Male', 'Female']\n",
      "Distinct values detected: 2\n",
      "\n",
      "  [Female]: 248 records\n",
      "  [Male]: 243 records\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Execute verification aggregation to confirm the remediation of the gender field for the formal audit trail\n",
    "# This ensures the 491-record baseline contains exactly two distinct values for gender\n",
    "gender_values_clean = list(collection.aggregate(pipeline_gender_consistency))\n",
    "\n",
    "print(\"--- Consistency Verification: applicant_info.gender ---\")\n",
    "print(\"Expected standard values: ['Male', 'Female']\")\n",
    "print(f\"Distinct values detected: {len(gender_values_clean)}\\n\")\n",
    "\n",
    "for gv in gender_values_clean:\n",
    "    # Handle potential null or missing values to prevent execution errors\n",
    "    val = gv['_id'] if gv['_id'] is not None else \"MISSING_OR_NULL\"\n",
    "    print(f\"  [{val}]: {gv['count']} records\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ac30d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Field: applicant_info.gender\n",
      "Distinct non-missing values: 2\n",
      "Expected: ['Male', 'Female']\n",
      "  'Female': 248\n",
      "  'Male': 243\n",
      "\n",
      "Field: loan_purpose\n",
      "Distinct non-missing values: 10\n",
      "Missing/Null records: 442\n",
      "  'medical': 8\n",
      "  'debt_consolidation': 6\n",
      "  'education': 6\n",
      "  'vacation': 6\n",
      "  'wedding': 6\n",
      "  'moving': 5\n",
      "  'personal': 4\n",
      "  'auto': 3\n",
      "  'home_improvement': 3\n",
      "  'business': 2\n",
      "\n",
      "Field: decision.loan_approved\n",
      "Distinct non-missing values: 2\n",
      "Expected: [True, False]\n",
      "  'True': 288\n",
      "  'False': 203\n",
      "\n",
      "Field: decision.rejection_reason\n",
      "Distinct non-missing values: 4\n",
      "Missing/Null records: 288\n",
      "  'algorithm_risk_score': 164\n",
      "  'insufficient_credit_history': 23\n",
      "  'high_dti_ratio': 12\n",
      "  'low_income': 4\n",
      "\n",
      "Field: spending_behavior.category\n",
      "Distinct non-missing values: 15\n",
      "  'Travel': 80\n",
      "  'Utilities': 74\n",
      "  'Fitness': 70\n",
      "  'Entertainment': 70\n",
      "  'Healthcare': 68\n",
      "  'Insurance': 65\n",
      "  'Education': 64\n",
      "  'Dining': 63\n",
      "  'Groceries': 62\n",
      "  'Rent': 59\n",
      "  'Transportation': 59\n",
      "  'Shopping': 53\n",
      "  'Alcohol': 11\n",
      "  'Gambling': 6\n",
      "  'Adult Entertainment': 5\n"
     ]
    }
   ],
   "source": [
    "# Check consistency for categorical fields in the current schema\n",
    "def check_field_consistency(field_path, expected_values=None, unwind_array_path=None):\n",
    "    \"\"\"Check how many distinct values exist for a field.\"\"\"\n",
    "    \n",
    "    # Fast schema guard: avoid misleading outputs when a field does not exist\n",
    "    existing_count = collection.count_documents({field_path: {\"$exists\": True, \"$ne\": None}})\n",
    "    print(f\"\\nField: {field_path}\")\n",
    "    if existing_count == 0:\n",
    "        print(\"Field not found (or always null) in current dataset schema.\")\n",
    "        return\n",
    "    \n",
    "    pipeline = []\n",
    "    if unwind_array_path:\n",
    "        pipeline.append({\"$unwind\": f\"${unwind_array_path}\"})\n",
    "    \n",
    "    pipeline.extend([\n",
    "        {\"$group\": {\"_id\": f\"${field_path}\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "])\n",
    "    results = list(collection.aggregate(pipeline))\n",
    "    \n",
    "    missing_count = sum(r['count'] for r in results if r['_id'] is None)\n",
    "    non_missing_results = [r for r in results if r['_id'] is not None]\n",
    "    \n",
    "    print(f\"Distinct non-missing values: {len(non_missing_results)}\")\n",
    "    if missing_count:\n",
    "        print(f\"Missing/Null records: {missing_count}\")\n",
    "    if expected_values:\n",
    "        print(f\"Expected: {expected_values}\")\n",
    "    for r in non_missing_results:\n",
    "        print(f\"  '{r['_id']}': {r['count']}\")\n",
    "\n",
    "# Check fields that exist in this dataset\n",
    "check_field_consistency(\"applicant_info.gender\", [\"Male\", \"Female\"])\n",
    "check_field_consistency(\"loan_purpose\")\n",
    "check_field_consistency(\"decision.loan_approved\", [True, False])\n",
    "check_field_consistency(\"decision.rejection_reason\")\n",
    "check_field_consistency(\"spending_behavior.category\", unwind_array_path=\"spending_behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4161e",
   "metadata": {},
   "source": [
    "The **Consistency Audit** confirms that demographic standardization is complete, with the gender field now partitioned into two balanced cohorts of **248 Female** and **243 Male** records. While the **loan_approved** field is **100% consistent**, the analysis exposed a critical **90.02% completeness failure** in **loan_purpose,** where 442 records are missing or null. Additionally, the audit successfully flagged high-risk behavioral data, including **Gambling** and **Adult Entertainment** within the spending arrays, though the systemic absence of **employment_status** indicates a remaining schema mapping error that prevents a full compliance review under **AI Act Art. 10**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
