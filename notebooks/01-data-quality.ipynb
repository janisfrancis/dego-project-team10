{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1337f7f",
   "metadata": {},
   "source": [
    "# 01 â€“ Data Quality Assessment\n",
    "\n",
    "This notebook analyses the raw credit application dataset for data quality issues across the following dimensions:\n",
    "\n",
    "- Completeness\n",
    "- Consistency\n",
    "- Validity\n",
    "- Accuracy\n",
    "\n",
    "All issues are quantified (counts and percentages) and mapped to governance implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6e826",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Structural Inspection\n",
    "- Load raw JSON\n",
    "- Inspect nested structure\n",
    "- Examine column names and data types\n",
    "\n",
    "## 2. Completeness Analysis\n",
    "- Missing values per column\n",
    "- Incomplete nested objects\n",
    "- % affected records\n",
    "\n",
    "## 3. Consistency & Type Validation\n",
    "- Data type mismatches\n",
    "- Inconsistent categorical encoding (e.g., gender formats)\n",
    "- Date format inconsistencies\n",
    "\n",
    "## 4. Validity Checks\n",
    "- Impossible values (e.g., negative income, negative credit history)\n",
    "- Logical inconsistencies (e.g., interest rate assigned when rejected)\n",
    "\n",
    "## 5. Duplicate Record Detection\n",
    "\n",
    "## 6. Remediation Strategy\n",
    "- Cleaning logic\n",
    "- Standardisation decisions\n",
    "- Governance implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc23179",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587f624",
   "metadata": {},
   "source": [
    "## Setup & Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2290625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/anaconda3/lib/python3.13/site-packages (4.16.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=2.6.1 in /opt/anaconda3/lib/python3.13/site-packages (from pymongo) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "!pip install pymongo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c73cd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: novacred. Dropped existing collection for fresh ingestion.\n",
      "\n",
      "--- Ingestion Audit Summary ---\n",
      "Standard Records Inserted: 500\n",
      "Duplicate IDs Rescued:    2\n",
      "Total Audit Baseline:      502 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "\n",
    "# Establish connection to the local MongoDB instance\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['novacred']\n",
    "collection = db['credit_applications']\n",
    "\n",
    "# Reset the collection to ensure an idempotent and clean baseline for the audit\n",
    "collection.drop()\n",
    "print(f\"Connected to: {db.name}. Dropped existing collection for fresh ingestion.\")\n",
    "\n",
    "# Determine the canonical data path based on the current working directory\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == 'notebooks':\n",
    "    data_path = cwd.parent / 'data' / 'raw_credit_applications.json'\n",
    "else:\n",
    "    data_path = cwd / 'data' / 'raw_credit_applications.json'\n",
    "\n",
    "# Validate the existence of the source file before proceeding\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"CRITICAL: Could not locate data file at: {data_path}\")\n",
    "\n",
    "# Load the raw application data from the JSON file\n",
    "with data_path.open('r') as file:\n",
    "    raw_data = json.load(file)\n",
    "\n",
    "# Execute the fault-tolerant ingestion loop\n",
    "successful_inserts = 0\n",
    "rescued_duplicates = 0\n",
    "\n",
    "for doc in raw_data:\n",
    "    try:\n",
    "        # Attempt to insert the document into the collection\n",
    "        collection.insert_one(doc)\n",
    "        successful_inserts += 1\n",
    "    except DuplicateKeyError:\n",
    "        # Append a suffix to the primary key to rescue colliding records for audit purposes\n",
    "        doc['_id'] = f\"{doc['_id']}_duplicate\"\n",
    "        collection.insert_one(doc)\n",
    "        rescued_duplicates += 1\n",
    "\n",
    "# Output the final ingestion metrics to verify the audit baseline\n",
    "print(\"\\n--- Ingestion Audit Summary ---\")\n",
    "print(f\"Standard Records Inserted: {successful_inserts}\")\n",
    "print(f\"Duplicate IDs Rescued:    {rescued_duplicates}\")\n",
    "print(f\"Total Audit Baseline:      {collection.count_documents({})} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9186f7e",
   "metadata": {},
   "source": [
    "**Ingestion Audit Note:**\n",
    " \n",
    "MongoDB automatically rejected 2 records (app_042, app_001) during import due to E11000 duplicate key errors on the _id field. To prevent data loss and ensure 100% auditability, these records were rescued by appending a _duplicate suffix to their _id. Baseline record count established at 502 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd16562",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76598b1",
   "metadata": {},
   "source": [
    "## Quick Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "67c1113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'app_200',\n",
      " 'applicant_info': {'date_of_birth': '2001-03-09',\n",
      "                    'email': 'jerry.smith17@hotmail.com',\n",
      "                    'full_name': 'Jerry Smith',\n",
      "                    'gender': 'Male',\n",
      "                    'ip_address': '192.168.48.155',\n",
      "                    'ssn': '596-64-4340',\n",
      "                    'zip_code': '10036'},\n",
      " 'decision': {'loan_approved': False,\n",
      "              'rejection_reason': 'algorithm_risk_score'},\n",
      " 'financials': {'annual_income': 73000,\n",
      "                'credit_history_months': 23,\n",
      "                'debt_to_income': 0.2,\n",
      "                'savings_balance': 31212},\n",
      " 'processing_timestamp': '2024-01-15T00:00:00Z',\n",
      " 'spending_behavior': [{'amount': 480, 'category': 'Shopping'},\n",
      "                       {'amount': 790, 'category': 'Rent'},\n",
      "                       {'amount': 247, 'category': 'Alcohol'}]}\n"
     ]
    }
   ],
   "source": [
    "# View a sample document from the collection\n",
    "sample = collection.find_one()\n",
    "pprint(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a0bef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089353a2",
   "metadata": {},
   "source": [
    "## Data Quality Dimension 1: Uniqueness\n",
    "\n",
    "The **Uniqueness Audit** identified **11 violations** across the 502-record dataset, establishing an initial health score of **97.81%** and a clean baseline of **491 records**. Findings included **two technical duplicates** for **Joseph Lopez** and **Stephanie Nguyen** flagged as \"**RESUBMISSION**\" and \"**DUPLICATE_ENTRY_ERROR**\", **four identity collisions** involving shared SSNs (e.g., Martinez/Wilson), and **five completeness failures** where SSNs were missing. All **11 records** were quarantined to satisfy **AI Act Art. 10** requirements for data accuracy and uniqueness, ensuring no redundant or ambiguous entries compromise the subsequent fairness audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27d48fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Identifiers (Completeness Issues) ---\n",
      " Found 5 records without an SSN:\n",
      "  - Margaret Williams (ID: app_075)\n",
      "  - Carolyn Martin (ID: app_120)\n",
      "  - Larry Williams (ID: app_268)\n",
      "  - Stephanie Nguyen (ID: app_001_duplicate)\n",
      "  - Brandon Moore (ID: app_165)\n",
      "\n",
      "--- Duplicate Identifiers (Uniqueness Issues) ---\n",
      "Found 3 duplicate SSNs (Identity Collisions):\n",
      " SSN: 780-24-9300 - Count: 2\n",
      "  - Susan Martinez (ID: app_088)\n",
      "  - Gary Wilson (ID: app_016)\n",
      "\n",
      " SSN: 652-70-5530 - Count: 2\n",
      "  - Joseph Lopez (ID: app_042)\n",
      "  - Joseph Lopez (ID: app_042_duplicate)\n",
      "\n",
      " SSN: 937-72-8731 - Count: 2\n",
      "  - Sandra Smith (ID: app_101)\n",
      "  - Samuel Hill (ID: app_234)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the aggregation pipeline to find duplicate SSNs - each person should appear only once!\n",
    "# This query specifically addresses the Uniqueness and Completeness dimensions for the 2026 Audit.\n",
    "pipeline_duplicates = [\n",
    "    {\n",
    "        # Group by the SSN identifier to detect identity collisions and missing values\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$applicant_info.ssn\",\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            # Map names and IDs into a records array to maintain a granular audit trail\n",
    "            \"records\": {\n",
    "                \"$push\": {\n",
    "                    \"name\": \"$applicant_info.full_name\",\n",
    "                    \"id\": \"$_id\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # Filter for groups that appear more than once to isolate potential data quality violations\n",
    "        \"$match\": {\n",
    "            \"count\": {\"$gt\": 1}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # Sort by frequency to prioritize the investigation of high-risk identifier collisions\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute the aggregation pipeline against the credit_applications collection\n",
    "duplicates = list(collection.aggregate(pipeline_duplicates))\n",
    "\n",
    "# Segregate results into Missing Identifiers and Duplicate Identifiers for focused governance reporting\n",
    "missing_ssn = [dup for dup in duplicates if dup['_id'] is None]\n",
    "actual_duplicates = [dup for dup in duplicates if dup['_id'] is not None]\n",
    "\n",
    "# Output findings for missing identifiers to address the Completeness requirement of the AI Act\n",
    "if missing_ssn:\n",
    "    print(\"--- Missing Identifiers (Completeness Issues) ---\")\n",
    "    for item in missing_ssn:\n",
    "        print(f\" Found {item['count']} records without an SSN:\")\n",
    "        for record in item['records']:\n",
    "            # List each specific record with its ID for technical validation\n",
    "            print(f\"  - {record['name']} (ID: {record['id']})\")\n",
    "        print()\n",
    "\n",
    "# Output findings for duplicate identifiers to address the Uniqueness requirement for high-risk AI data\n",
    "if actual_duplicates:\n",
    "    print(\"--- Duplicate Identifiers (Uniqueness Issues) ---\")\n",
    "    print(f\"Found {len(actual_duplicates)} duplicate SSNs (Identity Collisions):\")\n",
    "    for dup in actual_duplicates:\n",
    "        print(f\" SSN: {dup['_id']} - Count: {dup['count']}\")\n",
    "        for record in dup['records']:\n",
    "            # Document individual applicants sharing a single unique identifier\n",
    "            print(f\"  - {record['name']} (ID: {record['id']})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8417a4",
   "metadata": {},
   "source": [
    "The uniqueness audit of the **502-record baseline** identifies **11 violating documents** (2.19% of the dataset) across **four distinct failure groups**. These findings include **five applicants with missing SSN identifiers**, representing a critical **completeness failure**, and **six records** involved in **identity collisions** where **three SSNs** are shared by multiple entries. These collisions range from **exact system duplicates** (Joseph Lopez) to **conflicting identities** (Susan Martinez and Gary Wilson), suggesting both **system integration errors** and **potential fraud attempts** that violate **AI Act Art. 10** high-quality data standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b15e745c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Name-Based Collision Discovery (26 groups identified) ---\n",
      "\n",
      "Name: Susan Flores | Total Records: 3\n",
      "  - Record ID: app_448 | SSN: 383-48-9078\n",
      "  - Record ID: app_073 | SSN: 470-84-5617\n",
      "  - Record ID: app_226 | SSN: 817-96-6416\n",
      "--------------------------------------------------\n",
      "Name: Amy Flores | Total Records: 2\n",
      "  - Record ID: app_212 | SSN: 654-72-8456\n",
      "  - Record ID: app_146 | SSN: 577-59-1479\n",
      "--------------------------------------------------\n",
      "Name: Rachel King | Total Records: 2\n",
      "  - Record ID: app_193 | SSN: 852-24-1787\n",
      "  - Record ID: app_418 | SSN: 107-92-5280\n",
      "--------------------------------------------------\n",
      "Name: Shirley Davis | Total Records: 2\n",
      "  - Record ID: app_148 | SSN: 384-17-7019\n",
      "  - Record ID: app_219 | SSN: 994-53-6088\n",
      "--------------------------------------------------\n",
      "Name: James Rivera | Total Records: 2\n",
      "  - Record ID: app_465 | SSN: 853-96-1952\n",
      "  - Record ID: app_498 | SSN: 942-34-6834\n",
      "--------------------------------------------------\n",
      "Name: Jerry Nguyen | Total Records: 2\n",
      "  - Record ID: app_447 | SSN: 799-19-9143\n",
      "  - Record ID: app_231 | SSN: 821-93-7079\n",
      "--------------------------------------------------\n",
      "Name: Dorothy Perez | Total Records: 2\n",
      "  - Record ID: app_118 | SSN: 132-21-8550\n",
      "  - Record ID: app_426 | SSN: 402-41-6717\n",
      "--------------------------------------------------\n",
      "Name: Janet Johnson | Total Records: 2\n",
      "  - Record ID: app_131 | SSN: 605-97-2230\n",
      "  - Record ID: app_287 | SSN: 205-59-5076\n",
      "--------------------------------------------------\n",
      "Name: Stephanie Nguyen | Total Records: 2\n",
      "  - Record ID: app_001 | SSN: 427-90-1892\n",
      "  - Record ID: app_001_duplicate | SSN: MISSING\n",
      "--------------------------------------------------\n",
      "Name: James Martin | Total Records: 2\n",
      "  - Record ID: app_208 | SSN: 155-19-6776\n",
      "  - Record ID: app_302 | SSN: 594-58-9666\n",
      "--------------------------------------------------\n",
      "Name: Brandon Moore | Total Records: 2\n",
      "  - Record ID: app_134 | SSN: 714-64-1550\n",
      "  - Record ID: app_165 | SSN: MISSING\n",
      "--------------------------------------------------\n",
      "Name: George Garcia | Total Records: 2\n",
      "  - Record ID: app_107 | SSN: 565-34-4484\n",
      "  - Record ID: app_139 | SSN: 864-96-8449\n",
      "--------------------------------------------------\n",
      "Name: Susan Young | Total Records: 2\n",
      "  - Record ID: app_288 | SSN: 159-19-1765\n",
      "  - Record ID: app_321 | SSN: 395-66-6220\n",
      "--------------------------------------------------\n",
      "Name: Larry Williams | Total Records: 2\n",
      "  - Record ID: app_034 | SSN: 142-17-2501\n",
      "  - Record ID: app_268 | SSN: MISSING\n",
      "--------------------------------------------------\n",
      "Name: Joseph Lopez | Total Records: 2\n",
      "  - Record ID: app_042 | SSN: 652-70-5530\n",
      "  - Record ID: app_042_duplicate | SSN: 652-70-5530\n",
      "--------------------------------------------------\n",
      "Name: Mark Campbell | Total Records: 2\n",
      "  - Record ID: app_195 | SSN: 730-25-4224\n",
      "  - Record ID: app_110 | SSN: 223-83-4934\n",
      "--------------------------------------------------\n",
      "Name: Katherine Lopez | Total Records: 2\n",
      "  - Record ID: app_271 | SSN: 696-78-1129\n",
      "  - Record ID: app_047 | SSN: 241-14-6672\n",
      "--------------------------------------------------\n",
      "Name: Catherine Campbell | Total Records: 2\n",
      "  - Record ID: app_366 | SSN: 352-74-1939\n",
      "  - Record ID: app_087 | SSN: 499-29-5726\n",
      "--------------------------------------------------\n",
      "Name: Patrick Martinez | Total Records: 2\n",
      "  - Record ID: app_018 | SSN: 935-32-9047\n",
      "  - Record ID: app_468 | SSN: 996-86-6099\n",
      "--------------------------------------------------\n",
      "Name: Stephanie Allen | Total Records: 2\n",
      "  - Record ID: app_069 | SSN: 916-13-8007\n",
      "  - Record ID: app_257 | SSN: 509-17-2576\n",
      "--------------------------------------------------\n",
      "Name: Emma Clark | Total Records: 2\n",
      "  - Record ID: app_007 | SSN: 191-40-8079\n",
      "  - Record ID: app_463 | SSN: 976-47-3536\n",
      "--------------------------------------------------\n",
      "Name: George Clark | Total Records: 2\n",
      "  - Record ID: app_225 | SSN: 745-55-9936\n",
      "  - Record ID: app_185 | SSN: 804-59-4925\n",
      "--------------------------------------------------\n",
      "Name: Amanda Brown | Total Records: 2\n",
      "  - Record ID: app_050 | SSN: 695-46-5371\n",
      "  - Record ID: app_197 | SSN: 885-43-6142\n",
      "--------------------------------------------------\n",
      "Name: Brian Brown | Total Records: 2\n",
      "  - Record ID: app_169 | SSN: 566-85-7180\n",
      "  - Record ID: app_164 | SSN: 586-29-4288\n",
      "--------------------------------------------------\n",
      "Name: Heather Davis | Total Records: 2\n",
      "  - Record ID: app_367 | SSN: 618-61-5297\n",
      "  - Record ID: app_013 | SSN: 941-17-4706\n",
      "--------------------------------------------------\n",
      "Name: Anna White | Total Records: 2\n",
      "  - Record ID: app_377 | SSN: 617-17-3415\n",
      "  - Record ID: app_306 | SSN: 757-27-8131\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pipeline to find name collisions and their associated SSNs for secondary discovery\n",
    "pipeline_name_discovery = [\n",
    "    {\n",
    "        # Group by full name to identify homonym groups or missing-link duplicates\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$applicant_info.full_name\",\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            \"record_details\": {\n",
    "                \"$push\": {\n",
    "                    \"id\": \"$_id\",\n",
    "                    \"ssn\": \"$applicant_info.ssn\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # Filter for names appearing more than once to detect potential duplicates\n",
    "        \"$match\": {\"count\": {\"$gt\": 1}}\n",
    "    },\n",
    "    {\n",
    "        # Sort by frequency to prioritize the investigation of common name groups\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute discovery against the current audit collection\n",
    "name_collisions = list(collection.aggregate(pipeline_name_discovery))\n",
    "\n",
    "print(f\"--- Name-Based Collision Discovery ({len(name_collisions)} groups identified) ---\\n\")\n",
    "for group in name_collisions:\n",
    "    print(f\"Name: {group['_id']} | Total Records: {group['count']}\")\n",
    "    for record in group['record_details']:\n",
    "        ssn_value = record.get('ssn', 'MISSING')\n",
    "        print(f\"  - Record ID: {record['id']} | SSN: {ssn_value}\")\n",
    "    print(\"-\" * 50) # Visual separator for cleaner reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3afe2",
   "metadata": {},
   "source": [
    "The secondary discovery phase identified **26 groups** where names appear multiple times, totaling 53 records. Cross-referencing these with unique identifiers reveals that **23 groups** are **unique individuals** sharing common names (**homonyms**), such as Susan Flores, who must remain in the dataset. Only one group is a **confirmed system duplicate** (Joseph Lopez), while two others (Stephanie Nguyen, Brandon Moore) involve records with **missing identifiers**. These results confirm that **SSN-based auditing** is the only reliable deduplication method to avoid \"**Homonym Bias**\" and comply with **AI Act Art. 10** standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18587748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global Metadata Inspection (11 Records) ---\n",
      "\n",
      "ID: app_001 | Name: Stephanie Nguyen | SSN: 427-90-1892\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_001_duplicate | Name: Stephanie Nguyen | SSN: MISSING\n",
      "  -> [METADATA FOUND in 'notes']: DUPLICATE_ENTRY_ERROR\n",
      "--------------------------------------------------\n",
      "ID: app_016 | Name: Gary Wilson | SSN: 780-24-9300\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_042 | Name: Joseph Lopez | SSN: 652-70-5530\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_042_duplicate | Name: Joseph Lopez | SSN: 652-70-5530\n",
      "  -> [METADATA FOUND in 'notes']: RESUBMISSION\n",
      "--------------------------------------------------\n",
      "ID: app_075 | Name: Margaret Williams | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_088 | Name: Susan Martinez | SSN: 780-24-9300\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_101 | Name: Sandra Smith | SSN: 937-72-8731\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_120 | Name: Carolyn Martin | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_134 | Name: Brandon Moore | SSN: 714-64-1550\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_165 | Name: Brandon Moore | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_234 | Name: Samuel Hill | SSN: 937-72-8731\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_268 | Name: Larry Williams | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive list of all 11 records identified in the Uniqueness/Completeness audit\n",
    "all_violating_ids = [\n",
    "    \"app_042\", \"app_042_duplicate\", # Joseph Lopez (Confirmed Duplicate)\n",
    "    \"app_001\", \"app_001_duplicate\", # Stephanie Nguyen (Incomplete Link)\n",
    "    \"app_088\", \"app_016\",           # Martinez/Wilson (SSN Collision)\n",
    "    \"app_101\", \"app_234\",           # Smith/Hill (SSN Collision)\n",
    "    \"app_075\", \"app_120\", \"app_268\",# Williams/Martin/Williams (Missing IDs)\n",
    "    \"app_134\", \"app_165\"            # Brandon Moore (Incomplete Link)\n",
    "]\n",
    "\n",
    "# Retrieve the full documents for all flagged identifiers\n",
    "audit_details = list(collection.find({\"_id\": {\"$in\": all_violating_ids}}))\n",
    "\n",
    "print(f\"--- Global Metadata Inspection (11 Records) ---\\n\")\n",
    "for doc in audit_details:\n",
    "    name = doc.get('applicant_info', {}).get('full_name', 'Unknown')\n",
    "    ssn = doc.get('applicant_info', {}).get('ssn', 'MISSING')\n",
    "    \n",
    "    print(f\"ID: {doc['_id']} | Name: {name} | SSN: {ssn}\")\n",
    "    \n",
    "    # Dynamically scan for any field that might contain 'resubmission' or 'note'\n",
    "    found_metadata = False\n",
    "    for key, value in doc.items():\n",
    "        # Check top-level strings and nested dictionaries for 'note' or 'resubmit'\n",
    "        if any(term in str(key).lower() or term in str(value).lower() \n",
    "               for term in ['note', 'resubmit', 'audit', 'comment', 'flag']):\n",
    "            print(f\"  -> [METADATA FOUND in '{key}']: {value}\")\n",
    "            found_metadata = True\n",
    "            \n",
    "    if not found_metadata:\n",
    "        print(\"  -> No audit notes or system flags detected.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3bd0ad",
   "metadata": {},
   "source": [
    "The global metadata inspection confirms that two records contain explicit audit flags: **app_042_duplicate** (Joseph Lopez) is marked as a \"**RESUBMISSION**\" and **app_001_duplicate** (Stephanie Nguyen) as a \"**DUPLICATE_ENTRY_ERROR**\". These notes provide evidence of **system integration failures** for these specific cases. However, the remaining **nine records**, including the **identity collisions** (Martinez/Wilson and Smith/Hill) and **missing SSNs** (Williams, Martin, Moore), lack any explanatory metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87d85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Uniqueness Health Metric (Historical Baseline) ---\n",
      "Original Baseline Total: 502\n",
      "Total Records with Violations: 11\n",
      "Uniqueness Score: 97.81%\n"
     ]
    }
   ],
   "source": [
    "# Definitive list of the 11 violating records identified for the 2026 Audit\n",
    "target_quarantine_ids = [\n",
    "    \"app_042\", \"app_001_duplicate\", \"app_016\", \"app_088\", \n",
    "    \"app_101\", \"app_234\", \"app_075\", \"app_120\", \n",
    "    \"app_268\", \"app_134\", \"app_165\"\n",
    "]\n",
    "\n",
    "# Reconstruct the 502-record baseline count by checking both collections\n",
    "# This ensures the KPI remains accurate even after remediation\n",
    "active_count = collection.count_documents({})\n",
    "quarantined_count = db.quarantine_uniqueness.count_documents({\"_id\": {\"$in\": target_quarantine_ids}})\n",
    "baseline_total = active_count + quarantined_count\n",
    "\n",
    "# Calculate metrics based on the historical baseline to quantify data health\n",
    "violation_count = len(target_quarantine_ids)\n",
    "unique_score = ((baseline_total - violation_count) / baseline_total) * 100\n",
    "\n",
    "# Output the Uniqueness Health Metric for the audit report\n",
    "print(f\"--- Uniqueness Health Metric (Historical Baseline) ---\")\n",
    "print(f\"Original Baseline Total: {baseline_total}\")\n",
    "print(f\"Total Records with Violations: {violation_count}\")\n",
    "print(f\"Uniqueness Score: {unique_score:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52026f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Uniqueness Quarantine Verification (11 Records) ---\n",
      "ID                   | Name                 | Status/Note\n",
      "------------------------------------------------------------\n",
      "app_001_duplicate    | Stephanie Nguyen     | DUPLICATE_ENTRY_ERROR\n",
      "app_016              | Gary Wilson          | No specific flag (SSN Collision/Missing)\n",
      "app_042              | Joseph Lopez         | No specific flag (SSN Collision/Missing)\n",
      "app_075              | Margaret Williams    | No specific flag (SSN Collision/Missing)\n",
      "app_088              | Susan Martinez       | No specific flag (SSN Collision/Missing)\n",
      "app_101              | Sandra Smith         | No specific flag (SSN Collision/Missing)\n",
      "app_120              | Carolyn Martin       | No specific flag (SSN Collision/Missing)\n",
      "app_134              | Brandon Moore        | No specific flag (SSN Collision/Missing)\n",
      "app_165              | Brandon Moore        | No specific flag (SSN Collision/Missing)\n",
      "app_234              | Samuel Hill          | No specific flag (SSN Collision/Missing)\n",
      "app_268              | Larry Williams       | No specific flag (SSN Collision/Missing)\n",
      "\n",
      "Audit Status: Phase 1 (Uniqueness) Verified.\n",
      "Clean Baseline: 491 records remain in the audit pool.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all documents held in the quarantine collection for verification\n",
    "# This ensures the 11 identified violations are securely segregated from the active baseline\n",
    "quarantined_audit_trail = list(db.quarantine_uniqueness.find({}))\n",
    "\n",
    "# Output the Verification Table for the Audit Report to document the exclusion rationale\n",
    "print(f\"--- Uniqueness Quarantine Verification ({len(quarantined_audit_trail)} Records) ---\")\n",
    "print(f\"{'ID':<20} | {'Name':<20} | {'Status/Note'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for record in quarantined_audit_trail:\n",
    "    # Access the applicant name and relevant audit flags for reporting transparency\n",
    "    name = record.get('applicant_info', {}).get('full_name', 'Unknown')\n",
    "    # Capture the specific 'notes' field to prove the technical reason for quarantine\n",
    "    note = record.get('notes', 'No specific flag (SSN Collision/Missing)')\n",
    "    \n",
    "    print(f\"{record['_id']:<20} | {name:<20} | {note}\")\n",
    "\n",
    "# Final validation of the remaining active records to confirm readiness for the Consistency phase\n",
    "# The goal is to verify the transition from a 502-record set to a 491-record clean baseline\n",
    "print(f\"\\nAudit Status: Phase 1 (Uniqueness) Verified.\")\n",
    "print(f\"Clean Baseline: {collection.count_documents({})} records remain in the audit pool.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c340327a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea252a",
   "metadata": {},
   "source": [
    "## Data Quality Dimension 2: Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc157b0b",
   "metadata": {},
   "source": [
    "The **Categorical and Temporal Consistency Remediation** successfully resolved both encoding fragmentation and chronological anomalies within the 491-record baseline. The audit identified **109 records** using **non-standard abbreviations** (57 \"F\", 52 \"M\") and mapped them to the unified \"Female\" (248) and \"Male\" (243) taxonomy. \n",
    "\n",
    "To satisfy **AI Act Art. 10** and **12**, the cleaning pipeline quarantined **2 future-dated records** to preserve log traceability and **reconstructed the missing age feature** for the remaining 489 applicants. For applicant **app_350**, a **median age** was imputed to preserve its high-density financial data, resulting in a statistically robust 489-record baseline ready for subsequent risk modeling and fairness testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea715e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender value distribution:\n",
      "Expected: 2 distinct values (Male, Female)\n",
      "Actual: 4 distinct values\n",
      "\n",
      "  'Female': 191 records\n",
      "  'Male': 191 records\n",
      "  'F': 57 records\n",
      "  'M': 52 records\n"
     ]
    }
   ],
   "source": [
    "# How many different gender values exist?\n",
    "pipeline_gender_consistency = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$applicant_info.gender\",\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "gender_values = list(collection.aggregate(pipeline_gender_consistency))\n",
    "\n",
    "print(\"Gender value distribution:\")\n",
    "print(\"Expected: 2 distinct values (Male, Female)\")\n",
    "print(f\"Actual: {len(gender_values)} distinct values\")\n",
    "print()\n",
    "for gv in gender_values:\n",
    "    print(f\"  '{gv['_id']}': {gv['count']} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb9c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Categorical Remediation Executed ---\n",
      "Standardized 'M' -> 'Male': 0 records updated.\n",
      "Standardized 'F' -> 'Female': 0 records updated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute in-place database updates to resolve 109 categorical encoding fragmentation errors\n",
    "# Standardize gender values to align with the required taxonomy for AI Act Art. 10 fairness testing\n",
    "\n",
    "# Map 52 abbreviated 'M' encodings to the standard 'Male' category\n",
    "result_m = collection.update_many(\n",
    "    {\"applicant_info.gender\": \"M\"},\n",
    "    {\"$set\": {\"applicant_info.gender\": \"Male\"}}\n",
    ")\n",
    "\n",
    "# Map 57 abbreviated 'F' encodings to the standard 'Female' category\n",
    "result_f = collection.update_many(\n",
    "    {\"applicant_info.gender\": \"F\"},\n",
    "    {\"$set\": {\"applicant_info.gender\": \"Female\"}}\n",
    ")\n",
    "\n",
    "print(f\"--- Categorical Remediation Executed ---\")\n",
    "print(f\"Standardized 'M' -> 'Male': {result_m.modified_count} records updated.\")\n",
    "print(f\"Standardized 'F' -> 'Female': {result_f.modified_count} records updated.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0c21dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Consistency Verification: applicant_info.gender ---\n",
      "Expected standard values: ['Male', 'Female']\n",
      "Distinct values detected: 2\n",
      "\n",
      "  [Female]: 248 records\n",
      "  [Male]: 243 records\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Execute verification aggregation to confirm the remediation of the gender field for the formal audit trail\n",
    "# This ensures the 491-record baseline contains exactly two distinct values for gender\n",
    "gender_values_clean = list(collection.aggregate(pipeline_gender_consistency))\n",
    "\n",
    "print(\"--- Consistency Verification: applicant_info.gender ---\")\n",
    "print(\"Expected standard values: ['Male', 'Female']\")\n",
    "print(f\"Distinct values detected: {len(gender_values_clean)}\\n\")\n",
    "\n",
    "for gv in gender_values_clean:\n",
    "    # Handle potential null or missing values to prevent execution errors\n",
    "    val = gv['_id'] if gv['_id'] is not None else \"MISSING_OR_NULL\"\n",
    "    print(f\"  [{val}]: {gv['count']} records\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6ec66be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 3: Global Consistency & Categorical Audit ---\n",
      "\n",
      "Field Path: applicant_info.gender\n",
      "Distinct Values: 2\n",
      "Expected: ['Male', 'Female']\n",
      "  'Female': 248\n",
      "  'Male': 243\n",
      "\n",
      "Field Path: applicant_info.age\n",
      "Status: Field not found (or always null) in current dataset schema.\n",
      "\n",
      "Field Path: applicant_info.date_of_birth\n",
      "Distinct Values: 488\n",
      "  '1980-09-19': 2\n",
      "  '1965-11-07': 2\n",
      "  '1997-09-29': 2\n",
      "  '1983-05-10': 1\n",
      "  '1992-09-01': 1\n",
      "  '1999-08-26': 1\n",
      "  '1986-09-08': 1\n",
      "  '29/03/1963': 1\n",
      "  '25/01/1990': 1\n",
      "  '07/15/1999': 1\n",
      "  ... (+ 478 more distinct values)\n",
      "\n",
      "Field Path: financials.annual_income\n",
      "Distinct Values: 124\n",
      "  '79000': 11\n",
      "  '75000': 11\n",
      "  '100000': 10\n",
      "  '46000': 10\n",
      "  '80000': 9\n",
      "  '81000': 9\n",
      "  '98000': 9\n",
      "  '86000': 9\n",
      "  '74000': 8\n",
      "  '82000': 8\n",
      "  ... (+ 114 more distinct values)\n",
      "\n",
      "Field Path: financials.credit_history\n",
      "Status: Field not found (or always null) in current dataset schema.\n",
      "\n",
      "Field Path: loan_purpose\n",
      "Distinct Values: 10\n",
      "Missing/Null: 442 records\n",
      "  'medical': 8\n",
      "  'wedding': 6\n",
      "  'debt_consolidation': 6\n",
      "  'education': 6\n",
      "  'vacation': 6\n",
      "  'moving': 5\n",
      "  'personal': 4\n",
      "  'auto': 3\n",
      "  'home_improvement': 3\n",
      "  'business': 2\n",
      "\n",
      "Field Path: decision.loan_approved\n",
      "Distinct Values: 2\n",
      "Expected: [True, False]\n",
      "  'True': 288\n",
      "  'False': 203\n",
      "\n",
      "Field Path: decision.rejection_reason\n",
      "Distinct Values: 4\n",
      "Missing/Null: 288 records\n",
      "  'algorithm_risk_score': 164\n",
      "  'insufficient_credit_history': 23\n",
      "  'high_dti_ratio': 12\n",
      "  'low_income': 4\n",
      "\n",
      "Field Path: processing_timestamp\n",
      "Distinct Values: 4\n",
      "Missing/Null: 430 records\n",
      "  '2024-01-15T00:00:00Z': 58\n",
      "  '2025-12-01T00:00:00Z': 1\n",
      "  '2026-03-15T00:00:00Z': 1\n",
      "  '2027-01-20T00:00:00Z': 1\n",
      "\n",
      "Field Path: spending_behavior.category\n",
      "Distinct Values: 15\n",
      "  'Travel': 80\n",
      "  'Utilities': 74\n",
      "  'Entertainment': 70\n",
      "  'Fitness': 70\n",
      "  'Healthcare': 68\n",
      "  'Insurance': 65\n",
      "  'Education': 64\n",
      "  'Dining': 63\n",
      "  'Groceries': 62\n",
      "  'Rent': 59\n",
      "  ... (+ 5 more distinct values)\n",
      "\n",
      "Field Path: spending_behavior.amount\n",
      "Distinct Values: 523\n",
      "  '726': 6\n",
      "  '723': 5\n",
      "  '693': 5\n",
      "  '570': 5\n",
      "  '372': 5\n",
      "  '339': 4\n",
      "  '359': 4\n",
      "  '98': 4\n",
      "  '320': 4\n",
      "  '599': 4\n",
      "  ... (+ 513 more distinct values)\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Expanded Consistency Audit for all available fields\n",
    "def check_field_consistency(field_path, expected_values=None, unwind_array_path=None):\n",
    "    \"\"\"Check how many distinct values exist for a field and audit categorical density.\"\"\"\n",
    "    \n",
    "    # Fast schema guard: avoid misleading outputs when a field does not exist\n",
    "    existing_count = collection.count_documents({field_path: {\"$exists\": True, \"$ne\": None}})\n",
    "    print(f\"\\nField Path: {field_path}\")\n",
    "    if existing_count == 0:\n",
    "        print(\"Status: Field not found (or always null) in current dataset schema.\")\n",
    "        return\n",
    "    \n",
    "    pipeline = []\n",
    "    if unwind_array_path:\n",
    "        pipeline.append({\"$unwind\": f\"${unwind_array_path}\"})\n",
    "    \n",
    "    pipeline.extend([\n",
    "        {\"$group\": {\"_id\": f\"${field_path}\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ])\n",
    "    results = list(collection.aggregate(pipeline))\n",
    "    \n",
    "    missing_count = sum(r['count'] for r in results if r['_id'] is None)\n",
    "    non_missing_results = [r for r in results if r['_id'] is not None]\n",
    "    \n",
    "    print(f\"Distinct Values: {len(non_missing_results)}\")\n",
    "    if missing_count:\n",
    "        print(f\"Missing/Null: {missing_count} records\")\n",
    "    if expected_values:\n",
    "        print(f\"Expected: {expected_values}\")\n",
    "    \n",
    "    # Print only the top 10 values for high-variance fields to keep the audit scannable\n",
    "    for r in non_missing_results[:10]:\n",
    "        print(f\"  '{r['_id']}': {r['count']}\")\n",
    "    if len(non_missing_results) > 10:\n",
    "        print(f\"  ... (+ {len(non_missing_results) - 10} more distinct values)\")\n",
    "\n",
    "print(\"--- Phase 3: Global Consistency & Categorical Audit ---\")\n",
    "\n",
    "# 1. CORE IDENTITY & DEMOGRAPHICS\n",
    "check_field_consistency(\"applicant_info.gender\", [\"Male\", \"Female\"])\n",
    "check_field_consistency(\"applicant_info.age\")\n",
    "check_field_consistency(\"applicant_info.date_of_birth\") # Will show format inconsistency\n",
    "\n",
    "# 2. FINANCIAL & EMPLOYMENT ATTRIBUTES\n",
    "check_field_consistency(\"financials.annual_income\")\n",
    "check_field_consistency(\"financials.credit_history\", [\"Good\", \"Fair\", \"Poor\"])\n",
    "check_field_consistency(\"loan_purpose\") # Audit the 90% null rate context\n",
    "\n",
    "# 3. DECISION & GOVERNANCE METADATA\n",
    "check_field_consistency(\"decision.loan_approved\", [True, False])\n",
    "check_field_consistency(\"decision.rejection_reason\")\n",
    "check_field_consistency(\"processing_timestamp\") # Audit the 87.6% missing rate\n",
    "\n",
    "# 4. NESTED BEHAVIORAL ATTRIBUTES\n",
    "check_field_consistency(\"spending_behavior.category\", unwind_array_path=\"spending_behavior\")\n",
    "check_field_consistency(\"spending_behavior.amount\", unwind_array_path=\"spending_behavior\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4161e",
   "metadata": {},
   "source": [
    "The **Consistency Audit** confirms that demographic standardization is complete, with the gender field now partitioned into two balanced cohorts of **248 Female** and **243 Male** records. While the **loan_approved** field is **100% consistent**, the analysis exposed a critical **90.02% completeness failure** in **loan_purpose,** where 442 records are missing or null. Furthermore, **date_of_birth** exhibits severe **formatting fragmentation** across ISO, EU, and US standards, and the **processing_timestamp** contains a future-dated entry (2027), indicating **temporal data corruption**. Additionally, the audit successfully flagged high-risk behavioral data, including **Gambling** and **Adult Entertainment** within the spending arrays, though the systemic absence of **employment_status** indicates a remaining schema mapping error that prevents a full compliance review under **AI Act Art. 10**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9f1af425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timestamps Audited: 61 / 491\n",
      "Future-Dated Records: 2\n",
      "Corrupted Formats: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract logs and cast to UTC datetime for standardization\n",
    "cursor = list(collection.find({\"processing_timestamp\": {\"$exists\": True, \"$ne\": None}}))\n",
    "df_temp = pd.DataFrame(cursor)\n",
    "\n",
    "# Coerce errors to isolate corrupted strings instantly\n",
    "df_temp['parsed_time'] = pd.to_datetime(\n",
    "    df_temp['processing_timestamp'], \n",
    "    errors='coerce', \n",
    "    utc=True\n",
    ")\n",
    "\n",
    "# Define system present time to detect future data leakage\n",
    "audit_today = pd.Timestamp('2026-03-01', tz='UTC')\n",
    "\n",
    "# Filter for Art. 12 chronological violations\n",
    "future_records = df_temp[df_temp['parsed_time'] > audit_today]\n",
    "corrupted_records = df_temp[df_temp['parsed_time'].isna()]\n",
    "\n",
    "print(f\"Total Timestamps Audited: {len(df_temp)} / 491\")\n",
    "print(f\"Future-Dated Records: {len(future_records)}\")\n",
    "print(f\"Corrupted Formats: {len(corrupted_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d390dd2",
   "metadata": {},
   "source": [
    "The audit detected two transaction logs dated after March 1, 2026. This forward-looking data leakage is a critical breach of AI Act Art. 12, which requires accurate operational traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Engineering: Temporal Quarantine ---\n",
      "Records Quarantined (Future Dated): 2\n",
      "Remaining Valid Baseline Records:   489\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the audit boundary (current system date)\n",
    "audit_today = pd.Timestamp('2026-03-01', tz='UTC')\n",
    "\n",
    "# Convert processing_timestamp to datetime for logical comparison\n",
    "# errors='coerce' ensures we don't crash on nulls\n",
    "df_main['parsed_time'] = pd.to_datetime(df_main['processing_timestamp'], errors='coerce', utc=True)\n",
    "\n",
    "# Identify the boolean mask for Art. 12 violations (future dates)\n",
    "future_mask = df_main['parsed_time'] > audit_today\n",
    "\n",
    "# Split the dataset: Quarantine invalid records, retain valid/null records\n",
    "df_quarantine_temporal = df_main[future_mask].copy()\n",
    "df_main = df_main[~future_mask].copy()\n",
    "\n",
    "# Drop the temporary parsing column to keep the active schema clean\n",
    "df_main = df_main.drop(columns=['parsed_time'])\n",
    "\n",
    "print(\"--- Data Engineering: Temporal Quarantine ---\")\n",
    "print(f\"Records Quarantined (Future Dated): {len(df_quarantine_temporal)}\")\n",
    "print(f\"Remaining Valid Baseline Records:   {len(df_main)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2b77c",
   "metadata": {},
   "source": [
    "To remediate this without destroying evidence, both corrupted records were programmatically quarantined. This non-destructive exclusion successfully isolates the anomalies from the 59 valid baseline logs while preserving the raw data for regulatory review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ad6b0117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Engineering: Age Reconstruction Audit ---\n",
      "Valid DOB Formats Recovered: 490 / 491\n",
      "Age Features Engineered:     490 / 491\n",
      "Reconstructed Age Bounds:    23.0 to 67.0 years\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract the baseline and flatten the schema to instantiate df_main\n",
    "cursor = list(collection.find({}))\n",
    "df_main = pd.json_normalize(cursor)\n",
    "\n",
    "# Standardizes mixed US/EU/ISO date strings into unified datetime objects.\n",
    "# Corrupted or unparseable strings are safely converted to NaT via errors='coerce'.\n",
    "df_main['normalized_dob'] = pd.to_datetime(\n",
    "    df_main['applicant_info.date_of_birth'], \n",
    "    format='mixed', \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Establishes the 'Audit Present' to ensure calculations remain historically locked.\n",
    "audit_baseline = pd.Timestamp('2026-03-01')\n",
    "\n",
    "# Calculates exact timedelta, accounts for leap years, and applies floor function.\n",
    "df_main['engineered_age'] = np.floor(\n",
    "    (audit_baseline - df_main['normalized_dob']).dt.days / 365.25\n",
    ")\n",
    "\n",
    "print(\"--- Data Engineering: Age Reconstruction Audit ---\")\n",
    "print(f\"Valid DOB Formats Recovered: {df_main['normalized_dob'].notna().sum()} / 491\")\n",
    "print(f\"Age Features Engineered:     {df_main['engineered_age'].notna().sum()} / 491\")\n",
    "\n",
    "# Validating mathematical output to detect synthetic data anomalies.\n",
    "min_age = df_main['engineered_age'].min()\n",
    "max_age = df_main['engineered_age'].max()\n",
    "print(f\"Reconstructed Age Bounds:    {min_age} to {max_age} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "452d9d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Diagnostic Query: Comprehensive Metadata Extraction ---\n",
      "\n",
      "_id................................ app_350\n",
      "processing_timestamp............... \n",
      "applicant_info.full_name........... Linda Adams\n",
      "applicant_info.email............... \n",
      "applicant_info.ssn................. 356-98-8263\n",
      "applicant_info.ip_address.......... 10.207.183.196\n",
      "applicant_info.gender.............. Female\n",
      "applicant_info.date_of_birth....... \n",
      "applicant_info.zip_code............ 90291\n",
      "financials.annual_income........... 89000.0\n",
      "financials.credit_history_months... 52\n",
      "financials.debt_to_income.......... 0.2\n",
      "financials.savings_balance......... 14377\n",
      "decision.loan_approved............. True\n",
      "decision.rejection_reason.......... \n",
      "loan_purpose....................... \n",
      "decision.interest_rate............. 4.1\n",
      "decision.approved_amount........... 67000.0\n",
      "notes.............................. \n",
      "normalized_dob..................... \n",
      "engineered_age..................... \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filter the DataFrame for the specific row where the datetime conversion yielded NaT\n",
    "failed_record = df_main[df_main['normalized_dob'].isna()]\n",
    "\n",
    "print(\"--- Diagnostic Query: Comprehensive Metadata Extraction ---\\n\")\n",
    "\n",
    "if not failed_record.empty:\n",
    "    # Extract the single series representing the failed row without dropping NaN columns\n",
    "    record = failed_record.iloc[0]\n",
    "    \n",
    "    for col, val in record.items():\n",
    "        # Exclude the heavy nested array but intentionally keep the empty date_of_birth\n",
    "        if col != 'spending_behavior':\n",
    "            # Force Pandas NaT/NaN values to render as an empty string for clean output\n",
    "            display_val = \"\" if pd.isna(val) else val\n",
    "            print(f\"{col:.<35} {display_val}\")\n",
    "else:\n",
    "    print(\"No failed records found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a061e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Engineering: Median Age Imputation ---\n",
      "Target Record:                app_350\n",
      "Calculated Median Age:        39.0 years\n",
      "Missing Ages Post-Imputation: 0\n",
      "Total Baseline Records:       489\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pandas .median() automatically excludes NaT values during calculation\n",
    "median_age = df_main['engineered_age'].median()\n",
    "\n",
    "# Target the engineered_age column and replace the NaT value for app_350\n",
    "df_main['engineered_age'] = df_main['engineered_age'].fillna(median_age)\n",
    "\n",
    "missing_ages_after = df_main['engineered_age'].isna().sum()\n",
    "\n",
    "print(\"--- Data Engineering: Median Age Imputation ---\")\n",
    "print(f\"Target Record:                app_350\")\n",
    "print(f\"Calculated Median Age:        {median_age} years\")\n",
    "print(f\"Missing Ages Post-Imputation: {missing_ages_after}\")\n",
    "print(f\"Total Baseline Records:       {len(df_main)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e10574",
   "metadata": {},
   "source": [
    "To comply with AI Act Art. 10, median age imputation was performed on applicant app_350 to preserve a high-density financial profile ($89k income, 0.2 DTI) that would otherwise be lost to a structural schema void. The median was selected over the mean for its inherent resistance to outliers, ensuring the imputed value remains representative of the core demographic without warping the overall distribution. This engineering trade-off maintains the dataset's statistical integrity and maximizes the available training signals for subsequent risk modeling and fairness testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ffdb9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b1872",
   "metadata": {},
   "source": [
    "## Data Quality Dimension 3: Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9570e0c1",
   "metadata": {},
   "source": [
    "The Completeness Audit identifies a dataset with high integrity in core identity fields but significant operational voids that **jeopardize AI Act Art. 12 compliance**. While **SSN**, **Gender**, and **Credit History** maintain a **100% population rate**, an **87.6% missing rate** for **processing_timestamp** breaks the operational audit trail for **430 records**. \n",
    "\n",
    "Furthermore, structural fragmentation between **annual_income** and **annual_salary** indicates a \"**dirty schema**\" that prevents unified financial analysis. These gaps are compounded by a **100% absence of mandatory governance metadata** such as **data_source** and **retention_until** and a **90% null rate** in **loan_purpose**, leaving the baseline insufficient for the rigorous fairness and bias testing required for high-risk AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d05c3420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global Schema Discovery & Type Audit ---\n",
      "\n",
      "Total Columns Detected: 20\n",
      "\n",
      "_id.......................................... Records: 491/491 | Types: ['str']\n",
      "applicant_info.date_of_birth................. Records: 491/491 | Types: ['str']\n",
      "applicant_info.email......................... Records: 491/491 | Types: ['str']\n",
      "applicant_info.full_name..................... Records: 491/491 | Types: ['str']\n",
      "applicant_info.gender........................ Records: 491/491 | Types: ['str']\n",
      "applicant_info.ip_address.................... Records: 491/491 | Types: ['str']\n",
      "applicant_info.ssn........................... Records: 491/491 | Types: ['str']\n",
      "applicant_info.zip_code...................... Records: 491/491 | Types: ['str']\n",
      "decision.approved_amount..................... Records: 288/491 | Types: ['float']\n",
      "decision.interest_rate....................... Records: 288/491 | Types: ['float']\n",
      "decision.loan_approved....................... Records: 491/491 | Types: ['bool']\n",
      "decision.rejection_reason.................... Records: 203/491 | Types: ['str']\n",
      "financials.annual_income..................... Records: 491/491 | Types: ['float']\n",
      "financials.credit_history_months............. Records: 491/491 | Types: ['int']\n",
      "financials.debt_to_income.................... Records: 491/491 | Types: ['float']\n",
      "financials.savings_balance................... Records: 491/491 | Types: ['int']\n",
      "loan_purpose................................. Records:  49/491 | Types: ['str']\n",
      "notes........................................ Records:   1/491 | Types: ['str']\n",
      "processing_timestamp......................... Records:  61/491 | Types: ['str']\n",
      "spending_behavior............................ Records: 491/491 | Types: ['list']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Flatten the baseline and extract unique types per column\n",
    "df_flat = pd.json_normalize(list(collection.find({})))\n",
    "\n",
    "print(f\"--- Global Schema Discovery & Type Audit ---\\n\")\n",
    "print(f\"Total Columns Detected: {len(df_flat.columns)}\\n\")\n",
    "\n",
    "for col in sorted(df_flat.columns):\n",
    "    # Extract non-null data and identify unique Python types\n",
    "    valid_data = df_flat[col].dropna()\n",
    "    unique_types = sorted(valid_data.map(lambda x: type(x).__name__).unique()) if not valid_data.empty else [\"None\"]\n",
    "    \n",
    "    # Standardized output for Completeness (Counts) and Accuracy (Types)\n",
    "    print(f\"{col:.<45} Records: {len(valid_data):>3}/491 | Types: {unique_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433e4b0",
   "metadata": {},
   "source": [
    "The financial core is compromised by a \"**dirty schema**\" where income data is fragmented between **annual_income (486 records)** and **annual_salary (5 records)**, preventing unified feature analysis. Critically, the Accuracy Audit reveals a fatal **Type Inconsistency** within **annual_income**, which contains a contaminated **mix of ['float', 'int', 'str']**. These embedded string values represent a \"silent killer\" for model stability, as non-numeric inputs will crash mathematical pipelines. \n",
    "\n",
    "Furthermore, an **87.6% missing rate** for the **processing_timestamp** and a **100% absence of mandatory governance fields** like **data_source** break the operational audit trail for **430 records**, constituting a direct regulatory breach. Finally, the **90% null rate** in **loan_purpose** and the **total absence of employment_status** create significant data voids, leaving the dataset blind to variables required for meaningful fairness and bias testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ddde9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Nested Feature Discovery: spending_behavior ---\n",
      "\n",
      "amount....................................... Records: 809/809 | Types: ['int']\n",
      "category..................................... Records: 809/809 | Types: ['str']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Retrieve the baseline and specifically flatten the 'spending_behavior' list\n",
    "cursor = list(collection.find({}))\n",
    "df_spending = pd.json_normalize(\n",
    "    cursor, \n",
    "    record_path=['spending_behavior'], \n",
    "    meta=['_id', ['applicant_info', 'full_name']],\n",
    "    errors='ignore'\n",
    ")\n",
    "\n",
    "print(f\"--- Nested Feature Discovery: spending_behavior ---\\n\")\n",
    "\n",
    "# Iterate through the flattened sub-schema to audit data integrity at the transaction level\n",
    "for col in sorted(df_spending.columns):\n",
    "    # Filter non-null entries to identify the active data types and population counts\n",
    "    valid_data = df_spending[col].dropna()\n",
    "    non_null_count = len(valid_data)\n",
    "    total_entries = len(df_spending)\n",
    "    \n",
    "    # Identify unique native Python data types (e.g., str, int)\n",
    "    if not valid_data.empty:\n",
    "        unique_types = sorted(list(valid_data.map(lambda x: type(x).__name__).unique()))\n",
    "        type_str = f\"Types: {unique_types}\"\n",
    "    else:\n",
    "        type_str = \"Types: [None]\"\n",
    "    \n",
    "    # Exclude metadata columns to focus exclusively on the spending behavior attributes\n",
    "    if col not in ['_id', 'applicant_info.full_name']:\n",
    "        # Updated formatting to include the 'Records:' prefix within the alignment\n",
    "        print(f\"{col:.<45} Records: {len(valid_data)}/{total_entries} | {type_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4e09dd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Remediation Summary ---\n",
      "\n",
      "Records Merged (Salary -> Income): 0\n",
      "Records Cast (String -> Numeric): 0\n",
      "\n",
      "financials.annual_income Population: 491/491\n"
     ]
    }
   ],
   "source": [
    "# Move data from 'annual_salary' to 'annual_income' for the 5 fragmented records\n",
    "salary_fragment_update = collection.update_many(\n",
    "    {\"financials.annual_salary\": {\"$exists\": True}},\n",
    "    [\n",
    "        {\"$set\": {\"financials.annual_income\": \"$financials.annual_salary\"}},\n",
    "        {\"$unset\": \"financials.annual_salary\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Identify and convert all 'annual_income' string values to numeric (double/float)\n",
    "type_casting_update = collection.update_many(\n",
    "    {\"financials.annual_income\": {\"$type\": \"string\"}},\n",
    "    [\n",
    "        {\n",
    "            \"$set\": {\n",
    "                \"financials.annual_income\": {\n",
    "                    \"$convert\": {\n",
    "                        \"input\": \"$financials.annual_income\",\n",
    "                        \"to\": \"double\",\n",
    "                        \"onError\": None # Sets to null if conversion fails for audit review\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"--- Remediation Summary ---\\n\")\n",
    "print(f\"Records Merged (Salary -> Income): {salary_fragment_update.modified_count}\")\n",
    "print(f\"Records Cast (String -> Numeric): {type_casting_update.modified_count}\\n\")\n",
    "\n",
    "# Re-extract the baseline to verify final population and structural integrity\n",
    "df_verify = pd.json_normalize(list(collection.find({})))\n",
    "\n",
    "# Verify final population count for 'annual_income'\n",
    "income_count = df_verify[\"financials.annual_income\"].notna().sum()\n",
    "print(f\"financials.annual_income Population: {income_count}/491\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "33a6257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Traceability Recovery Audit ---\n",
      "\n",
      "Successfully recovered 0 proxy timestamps.\n",
      "\n",
      "Sample Recovery Trail (First 5 Records):\n",
      "- ID: app_037                   | Proxy Time: None\n",
      "- ID: app_215                   | Proxy Time: None\n",
      "- ID: app_024                   | Proxy Time: None\n",
      "- ID: app_275                   | Proxy Time: None\n",
      "- ID: app_099                   | Proxy Time: None\n"
     ]
    }
   ],
   "source": [
    "from bson import ObjectId\n",
    "import pandas as pd\n",
    "\n",
    "cursor = list(collection.find({\"processing_timestamp\": {\"$exists\": False}}))\n",
    "df_missing = pd.DataFrame(cursor)\n",
    "\n",
    "print(f\"--- Traceability Recovery Audit ---\\n\")\n",
    "\n",
    "# Extract the 'hidden' creation time from the _id metadata\n",
    "def recover_timestamp(doc_id):\n",
    "    try:\n",
    "        # Cast string to BSON ObjectId to access generation_time\n",
    "        return ObjectId(doc_id).generation_time\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the recovery to create a proxy audit trail\n",
    "df_missing['proxy_timestamp'] = df_missing['_id'].apply(recover_timestamp)\n",
    "\n",
    "print(f\"Successfully recovered {df_missing['proxy_timestamp'].notna().sum()} proxy timestamps.\\n\")\n",
    "print(f\"Sample Recovery Trail (First 5 Records):\")\n",
    "\n",
    "# Standardized alignment for professional audit reporting\n",
    "for _, row in df_missing.head(5).iterrows():\n",
    "    print(f\"- ID: {str(row['_id']):<25} | Proxy Time: {row['proxy_timestamp']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd905e2",
   "metadata": {},
   "source": [
    "A Metadata Recovery Audit was performed to mitigate the **87.6% missing rate** in **processing_timestamp**. Attempts to extract creation metadata from the **_id** field confirmed that the dataset utilizes **Custom Application Strings** (e.g., app_037) rather than native BSON ObjectIds. This architectural choice renders the operational audit trail **permanently unrecoverable**, constituting a definitive breach of **AI Act Art. 12** logging requirements for high-risk AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "338710ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Governance Field Completeness Check:\n",
      "============================================================\n",
      "ðŸ”´ processing_timestamp: 430 missing (Operational traceability timestamp)\n",
      "ðŸ”´ retention_until: 491 missing (GDPR Art. 5 - Storage Limitation)\n",
      "ðŸ”´ data_source: 491 missing (GDPR Art. 14 - Transparency)\n",
      "ðŸ”´ processing_purpose: 491 missing (GDPR Art. 5 - Purpose Limitation)\n"
     ]
    }
   ],
   "source": [
    "# Check other governance-critical fields\n",
    "governance_fields = [\n",
    "    (\"processing_timestamp\", \"Operational traceability timestamp\"),\n",
    "    (\"retention_until\", \"GDPR Art. 5 - Storage Limitation\"),\n",
    "    (\"data_source\", \"GDPR Art. 14 - Transparency\"),\n",
    "    (\"processing_purpose\", \"GDPR Art. 5 - Purpose Limitation\")\n",
    "]\n",
    "\n",
    "print(\"\\nGovernance Field Completeness Check:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for field, governance_note in governance_fields:\n",
    "    pipeline = [\n",
    "        {\"$match\": {field: {\"$exists\": False}}},\n",
    "        {\"$count\": \"missing\"}\n",
    "    ]\n",
    "    result = list(collection.aggregate(pipeline))\n",
    "    missing = result[0]['missing'] if result else 0\n",
    "    \n",
    "    status = \"âœ…\" if missing == 0 else \"ðŸ”´\"\n",
    "    print(f\"{status} {field}: {missing} missing ({governance_note})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7edd6",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
