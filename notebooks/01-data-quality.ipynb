{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1337f7f",
   "metadata": {},
   "source": [
    "# 01 â€“ Data Quality Assessment\n",
    "\n",
    "This notebook analyses the raw credit application dataset for data quality issues across the following dimensions:\n",
    "\n",
    "- Completeness\n",
    "- Consistency\n",
    "- Validity\n",
    "- Accuracy\n",
    "\n",
    "All issues are quantified (counts and percentages) and mapped to governance implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6e826",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Structural Inspection\n",
    "- Load raw JSON\n",
    "- Inspect nested structure\n",
    "- Examine column names and data types\n",
    "\n",
    "## 2. Completeness Analysis\n",
    "- Missing values per column\n",
    "- Incomplete nested objects\n",
    "- % affected records\n",
    "\n",
    "## 3. Consistency & Type Validation\n",
    "- Data type mismatches\n",
    "- Inconsistent categorical encoding (e.g., gender formats)\n",
    "- Date format inconsistencies\n",
    "\n",
    "## 4. Validity Checks\n",
    "- Impossible values (e.g., negative income, negative credit history)\n",
    "- Logical inconsistencies (e.g., interest rate assigned when rejected)\n",
    "\n",
    "## 5. Duplicate Record Detection\n",
    "\n",
    "## 6. Remediation Strategy\n",
    "- Cleaning logic\n",
    "- Standardisation decisions\n",
    "- Governance implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc23179",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587f624",
   "metadata": {},
   "source": [
    "## Setup & Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2290625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/anaconda3/lib/python3.13/site-packages (4.16.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=2.6.1 in /opt/anaconda3/lib/python3.13/site-packages (from pymongo) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "!pip install pymongo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c73cd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: novacred. Dropped existing collection for fresh ingestion.\n",
      "\n",
      "--- Ingestion Audit Summary ---\n",
      "Standard Records Inserted: 500\n",
      "Duplicate IDs Rescued:    2\n",
      "Total Audit Baseline:      502 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "\n",
    "# Establish connection to the local MongoDB instance\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['novacred']\n",
    "collection = db['credit_applications']\n",
    "\n",
    "# Reset the collection to ensure an idempotent and clean baseline for the audit\n",
    "collection.drop()\n",
    "print(f\"Connected to: {db.name}. Dropped existing collection for fresh ingestion.\")\n",
    "\n",
    "# Determine the canonical data path based on the current working directory\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == 'notebooks':\n",
    "    data_path = cwd.parent / 'data' / 'raw_credit_applications.json'\n",
    "else:\n",
    "    data_path = cwd / 'data' / 'raw_credit_applications.json'\n",
    "\n",
    "# Validate the existence of the source file before proceeding\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"CRITICAL: Could not locate data file at: {data_path}\")\n",
    "\n",
    "# Load the raw application data from the JSON file\n",
    "with data_path.open('r') as file:\n",
    "    raw_data = json.load(file)\n",
    "\n",
    "# Execute the fault-tolerant ingestion loop\n",
    "successful_inserts = 0\n",
    "rescued_duplicates = 0\n",
    "\n",
    "for doc in raw_data:\n",
    "    try:\n",
    "        # Attempt to insert the document into the collection\n",
    "        collection.insert_one(doc)\n",
    "        successful_inserts += 1\n",
    "    except DuplicateKeyError:\n",
    "        # Append a suffix to the primary key to rescue colliding records for audit purposes\n",
    "        doc['_id'] = f\"{doc['_id']}_duplicate\"\n",
    "        collection.insert_one(doc)\n",
    "        rescued_duplicates += 1\n",
    "\n",
    "# Output the final ingestion metrics to verify the audit baseline\n",
    "print(\"\\n--- Ingestion Audit Summary ---\")\n",
    "print(f\"Standard Records Inserted: {successful_inserts}\")\n",
    "print(f\"Duplicate IDs Rescued:    {rescued_duplicates}\")\n",
    "print(f\"Total Audit Baseline:      {collection.count_documents({})} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9186f7e",
   "metadata": {},
   "source": [
    "**Ingestion Audit Note:**\n",
    " \n",
    "MongoDB automatically rejected 2 records (app_042, app_001) during import due to E11000 duplicate key errors on the _id field. To prevent data loss and ensure 100% auditability, these records were rescued by appending a _duplicate suffix to their _id. Baseline record count established at 502 documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd16562",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76598b1",
   "metadata": {},
   "source": [
    "## Quick Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67c1113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'app_200',\n",
      " 'applicant_info': {'date_of_birth': '2001-03-09',\n",
      "                    'email': 'jerry.smith17@hotmail.com',\n",
      "                    'full_name': 'Jerry Smith',\n",
      "                    'gender': 'Male',\n",
      "                    'ip_address': '192.168.48.155',\n",
      "                    'ssn': '596-64-4340',\n",
      "                    'zip_code': '10036'},\n",
      " 'decision': {'loan_approved': False,\n",
      "              'rejection_reason': 'algorithm_risk_score'},\n",
      " 'financials': {'annual_income': 73000,\n",
      "                'credit_history_months': 23,\n",
      "                'debt_to_income': 0.2,\n",
      "                'savings_balance': 31212},\n",
      " 'processing_timestamp': '2024-01-15T00:00:00Z',\n",
      " 'spending_behavior': [{'amount': 480, 'category': 'Shopping'},\n",
      "                       {'amount': 790, 'category': 'Rent'},\n",
      "                       {'amount': 247, 'category': 'Alcohol'}]}\n"
     ]
    }
   ],
   "source": [
    "# View a sample document from the collection\n",
    "sample = collection.find_one()\n",
    "pprint(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a0bef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089353a2",
   "metadata": {},
   "source": [
    "## Audit Query 1: Find Duplicates | Data Quality Dimension: Uniqueness\n",
    "\n",
    "Potential issue: The same person is appearing multiple times in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27d48fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Identifiers (Completeness Issues) ---\n",
      " Found 5 records without an SSN:\n",
      "  - Margaret Williams (ID: app_075)\n",
      "  - Carolyn Martin (ID: app_120)\n",
      "  - Larry Williams (ID: app_268)\n",
      "  - Stephanie Nguyen (ID: app_001_duplicate)\n",
      "  - Brandon Moore (ID: app_165)\n",
      "\n",
      "--- Duplicate Identifiers (Uniqueness Issues) ---\n",
      "Found 3 duplicate SSNs (Identity Collisions):\n",
      " SSN: 780-24-9300 - Count: 2\n",
      "  - Susan Martinez (ID: app_088)\n",
      "  - Gary Wilson (ID: app_016)\n",
      "\n",
      " SSN: 652-70-5530 - Count: 2\n",
      "  - Joseph Lopez (ID: app_042)\n",
      "  - Joseph Lopez (ID: app_042_duplicate)\n",
      "\n",
      " SSN: 937-72-8731 - Count: 2\n",
      "  - Sandra Smith (ID: app_101)\n",
      "  - Samuel Hill (ID: app_234)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the aggregation pipeline to find duplicate SSNs - each person should appear only once!\n",
    "# This query specifically addresses the Uniqueness and Completeness dimensions for the 2026 Audit.\n",
    "pipeline_duplicates = [\n",
    "    {\n",
    "        # Group by the SSN identifier to detect identity collisions and missing values\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$applicant_info.ssn\",\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            # Map names and IDs into a records array to maintain a granular audit trail\n",
    "            \"records\": {\n",
    "                \"$push\": {\n",
    "                    \"name\": \"$applicant_info.full_name\",\n",
    "                    \"id\": \"$_id\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # Filter for groups that appear more than once to isolate potential data quality violations\n",
    "        \"$match\": {\n",
    "            \"count\": {\"$gt\": 1}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # Sort by frequency to prioritize the investigation of high-risk identifier collisions\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute the aggregation pipeline against the credit_applications collection\n",
    "duplicates = list(collection.aggregate(pipeline_duplicates))\n",
    "\n",
    "# Segregate results into Missing Identifiers and Duplicate Identifiers for focused governance reporting\n",
    "missing_ssn = [dup for dup in duplicates if dup['_id'] is None]\n",
    "actual_duplicates = [dup for dup in duplicates if dup['_id'] is not None]\n",
    "\n",
    "# Output findings for missing identifiers to address the Completeness requirement of the AI Act\n",
    "if missing_ssn:\n",
    "    print(\"--- Missing Identifiers (Completeness Issues) ---\")\n",
    "    for item in missing_ssn:\n",
    "        print(f\" Found {item['count']} records without an SSN:\")\n",
    "        for record in item['records']:\n",
    "            # List each specific record with its ID for technical validation\n",
    "            print(f\"  - {record['name']} (ID: {record['id']})\")\n",
    "        print()\n",
    "\n",
    "# Output findings for duplicate identifiers to address the Uniqueness requirement for high-risk AI data\n",
    "if actual_duplicates:\n",
    "    print(\"--- Duplicate Identifiers (Uniqueness Issues) ---\")\n",
    "    print(f\"Found {len(actual_duplicates)} duplicate SSNs (Identity Collisions):\")\n",
    "    for dup in actual_duplicates:\n",
    "        print(f\" SSN: {dup['_id']} - Count: {dup['count']}\")\n",
    "        for record in dup['records']:\n",
    "            # Document individual applicants sharing a single unique identifier\n",
    "            print(f\"  - {record['name']} (ID: {record['id']})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8417a4",
   "metadata": {},
   "source": [
    "The uniqueness audit of the **502-record baseline** identifies **11 violating documents** (2.19% of the dataset) across **four distinct failure groups**. These findings include **five applicants with missing SSN identifiers**, representing a critical **completeness failure**, and **six records** involved in **identity collisions** where **three SSNs** are shared by multiple entries. These collisions range from **exact system duplicates** (Joseph Lopez) to **conflicting identities** (Susan Martinez and Gary Wilson), suggesting both **system integration errors** and **potential fraud attempts** that violate **AI Act Art. 10** high-quality data standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b15e745c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Name-Based Collision Discovery (26 groups identified) ---\n",
      "\n",
      "Name: Susan Flores | Total Records: 3\n",
      "  - Record ID: app_448 | SSN: 383-48-9078\n",
      "  - Record ID: app_073 | SSN: 470-84-5617\n",
      "  - Record ID: app_226 | SSN: 817-96-6416\n",
      "--------------------------------------------------\n",
      "Name: Amy Flores | Total Records: 2\n",
      "  - Record ID: app_212 | SSN: 654-72-8456\n",
      "  - Record ID: app_146 | SSN: 577-59-1479\n",
      "--------------------------------------------------\n",
      "Name: Rachel King | Total Records: 2\n",
      "  - Record ID: app_193 | SSN: 852-24-1787\n",
      "  - Record ID: app_418 | SSN: 107-92-5280\n",
      "--------------------------------------------------\n",
      "Name: Shirley Davis | Total Records: 2\n",
      "  - Record ID: app_148 | SSN: 384-17-7019\n",
      "  - Record ID: app_219 | SSN: 994-53-6088\n",
      "--------------------------------------------------\n",
      "Name: James Rivera | Total Records: 2\n",
      "  - Record ID: app_465 | SSN: 853-96-1952\n",
      "  - Record ID: app_498 | SSN: 942-34-6834\n",
      "--------------------------------------------------\n",
      "Name: Jerry Nguyen | Total Records: 2\n",
      "  - Record ID: app_447 | SSN: 799-19-9143\n",
      "  - Record ID: app_231 | SSN: 821-93-7079\n",
      "--------------------------------------------------\n",
      "Name: Dorothy Perez | Total Records: 2\n",
      "  - Record ID: app_118 | SSN: 132-21-8550\n",
      "  - Record ID: app_426 | SSN: 402-41-6717\n",
      "--------------------------------------------------\n",
      "Name: Janet Johnson | Total Records: 2\n",
      "  - Record ID: app_131 | SSN: 605-97-2230\n",
      "  - Record ID: app_287 | SSN: 205-59-5076\n",
      "--------------------------------------------------\n",
      "Name: Stephanie Nguyen | Total Records: 2\n",
      "  - Record ID: app_001 | SSN: 427-90-1892\n",
      "  - Record ID: app_001_duplicate | SSN: MISSING\n",
      "--------------------------------------------------\n",
      "Name: James Martin | Total Records: 2\n",
      "  - Record ID: app_208 | SSN: 155-19-6776\n",
      "  - Record ID: app_302 | SSN: 594-58-9666\n",
      "--------------------------------------------------\n",
      "Name: Brandon Moore | Total Records: 2\n",
      "  - Record ID: app_134 | SSN: 714-64-1550\n",
      "  - Record ID: app_165 | SSN: MISSING\n",
      "--------------------------------------------------\n",
      "Name: George Garcia | Total Records: 2\n",
      "  - Record ID: app_107 | SSN: 565-34-4484\n",
      "  - Record ID: app_139 | SSN: 864-96-8449\n",
      "--------------------------------------------------\n",
      "Name: Susan Young | Total Records: 2\n",
      "  - Record ID: app_288 | SSN: 159-19-1765\n",
      "  - Record ID: app_321 | SSN: 395-66-6220\n",
      "--------------------------------------------------\n",
      "Name: Larry Williams | Total Records: 2\n",
      "  - Record ID: app_034 | SSN: 142-17-2501\n",
      "  - Record ID: app_268 | SSN: MISSING\n",
      "--------------------------------------------------\n",
      "Name: Joseph Lopez | Total Records: 2\n",
      "  - Record ID: app_042 | SSN: 652-70-5530\n",
      "  - Record ID: app_042_duplicate | SSN: 652-70-5530\n",
      "--------------------------------------------------\n",
      "Name: Mark Campbell | Total Records: 2\n",
      "  - Record ID: app_195 | SSN: 730-25-4224\n",
      "  - Record ID: app_110 | SSN: 223-83-4934\n",
      "--------------------------------------------------\n",
      "Name: Katherine Lopez | Total Records: 2\n",
      "  - Record ID: app_271 | SSN: 696-78-1129\n",
      "  - Record ID: app_047 | SSN: 241-14-6672\n",
      "--------------------------------------------------\n",
      "Name: Catherine Campbell | Total Records: 2\n",
      "  - Record ID: app_366 | SSN: 352-74-1939\n",
      "  - Record ID: app_087 | SSN: 499-29-5726\n",
      "--------------------------------------------------\n",
      "Name: Patrick Martinez | Total Records: 2\n",
      "  - Record ID: app_018 | SSN: 935-32-9047\n",
      "  - Record ID: app_468 | SSN: 996-86-6099\n",
      "--------------------------------------------------\n",
      "Name: Stephanie Allen | Total Records: 2\n",
      "  - Record ID: app_069 | SSN: 916-13-8007\n",
      "  - Record ID: app_257 | SSN: 509-17-2576\n",
      "--------------------------------------------------\n",
      "Name: Emma Clark | Total Records: 2\n",
      "  - Record ID: app_007 | SSN: 191-40-8079\n",
      "  - Record ID: app_463 | SSN: 976-47-3536\n",
      "--------------------------------------------------\n",
      "Name: George Clark | Total Records: 2\n",
      "  - Record ID: app_225 | SSN: 745-55-9936\n",
      "  - Record ID: app_185 | SSN: 804-59-4925\n",
      "--------------------------------------------------\n",
      "Name: Amanda Brown | Total Records: 2\n",
      "  - Record ID: app_050 | SSN: 695-46-5371\n",
      "  - Record ID: app_197 | SSN: 885-43-6142\n",
      "--------------------------------------------------\n",
      "Name: Brian Brown | Total Records: 2\n",
      "  - Record ID: app_169 | SSN: 566-85-7180\n",
      "  - Record ID: app_164 | SSN: 586-29-4288\n",
      "--------------------------------------------------\n",
      "Name: Heather Davis | Total Records: 2\n",
      "  - Record ID: app_367 | SSN: 618-61-5297\n",
      "  - Record ID: app_013 | SSN: 941-17-4706\n",
      "--------------------------------------------------\n",
      "Name: Anna White | Total Records: 2\n",
      "  - Record ID: app_377 | SSN: 617-17-3415\n",
      "  - Record ID: app_306 | SSN: 757-27-8131\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pipeline to find name collisions and their associated SSNs for secondary discovery\n",
    "pipeline_name_discovery = [\n",
    "    {\n",
    "        # Group by full name to identify homonym groups or missing-link duplicates\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$applicant_info.full_name\",\n",
    "            \"count\": {\"$sum\": 1},\n",
    "            \"record_details\": {\n",
    "                \"$push\": {\n",
    "                    \"id\": \"$_id\",\n",
    "                    \"ssn\": \"$applicant_info.ssn\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # Filter for names appearing more than once to detect potential duplicates\n",
    "        \"$match\": {\"count\": {\"$gt\": 1}}\n",
    "    },\n",
    "    {\n",
    "        # Sort by frequency to prioritize the investigation of common name groups\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute discovery against the current audit collection\n",
    "name_collisions = list(collection.aggregate(pipeline_name_discovery))\n",
    "\n",
    "print(f\"--- Name-Based Collision Discovery ({len(name_collisions)} groups identified) ---\\n\")\n",
    "for group in name_collisions:\n",
    "    print(f\"Name: {group['_id']} | Total Records: {group['count']}\")\n",
    "    for record in group['record_details']:\n",
    "        ssn_value = record.get('ssn', 'MISSING')\n",
    "        print(f\"  - Record ID: {record['id']} | SSN: {ssn_value}\")\n",
    "    print(\"-\" * 50) # Visual separator for cleaner reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3afe2",
   "metadata": {},
   "source": [
    "The secondary discovery phase identified **26 groups** where names appear multiple times, totaling 53 records. Cross-referencing these with unique identifiers reveals that **23 groups** are **unique individuals** sharing common names (**homonyms**), such as Susan Flores, who must remain in the dataset. Only one group is a **confirmed system duplicate** (Joseph Lopez), while two others (Stephanie Nguyen, Brandon Moore) involve records with **missing identifiers**. These results confirm that **SSN-based auditing** is the only reliable deduplication method to avoid \"**Homonym Bias**\" and comply with **AI Act Art. 10** standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18587748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global Metadata Inspection (11 Records) ---\n",
      "\n",
      "ID: app_001 | Name: Stephanie Nguyen | SSN: 427-90-1892\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_001_duplicate | Name: Stephanie Nguyen | SSN: MISSING\n",
      "  -> [METADATA FOUND in 'notes']: DUPLICATE_ENTRY_ERROR\n",
      "--------------------------------------------------\n",
      "ID: app_016 | Name: Gary Wilson | SSN: 780-24-9300\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_042 | Name: Joseph Lopez | SSN: 652-70-5530\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_042_duplicate | Name: Joseph Lopez | SSN: 652-70-5530\n",
      "  -> [METADATA FOUND in 'notes']: RESUBMISSION\n",
      "--------------------------------------------------\n",
      "ID: app_075 | Name: Margaret Williams | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_088 | Name: Susan Martinez | SSN: 780-24-9300\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_101 | Name: Sandra Smith | SSN: 937-72-8731\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_120 | Name: Carolyn Martin | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_134 | Name: Brandon Moore | SSN: 714-64-1550\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_165 | Name: Brandon Moore | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_234 | Name: Samuel Hill | SSN: 937-72-8731\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n",
      "ID: app_268 | Name: Larry Williams | SSN: MISSING\n",
      "  -> No audit notes or system flags detected.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive list of all 11 records identified in the Uniqueness/Completeness audit\n",
    "all_violating_ids = [\n",
    "    \"app_042\", \"app_042_duplicate\", # Joseph Lopez (Confirmed Duplicate)\n",
    "    \"app_001\", \"app_001_duplicate\", # Stephanie Nguyen (Incomplete Link)\n",
    "    \"app_088\", \"app_016\",           # Martinez/Wilson (SSN Collision)\n",
    "    \"app_101\", \"app_234\",           # Smith/Hill (SSN Collision)\n",
    "    \"app_075\", \"app_120\", \"app_268\",# Williams/Martin/Williams (Missing IDs)\n",
    "    \"app_134\", \"app_165\"            # Brandon Moore (Incomplete Link)\n",
    "]\n",
    "\n",
    "# Retrieve the full documents for all flagged identifiers\n",
    "audit_details = list(collection.find({\"_id\": {\"$in\": all_violating_ids}}))\n",
    "\n",
    "print(f\"--- Global Metadata Inspection (11 Records) ---\\n\")\n",
    "for doc in audit_details:\n",
    "    name = doc.get('applicant_info', {}).get('full_name', 'Unknown')\n",
    "    ssn = doc.get('applicant_info', {}).get('ssn', 'MISSING')\n",
    "    \n",
    "    print(f\"ID: {doc['_id']} | Name: {name} | SSN: {ssn}\")\n",
    "    \n",
    "    # Dynamically scan for any field that might contain 'resubmission' or 'note'\n",
    "    found_metadata = False\n",
    "    for key, value in doc.items():\n",
    "        # Check top-level strings and nested dictionaries for 'note' or 'resubmit'\n",
    "        if any(term in str(key).lower() or term in str(value).lower() \n",
    "               for term in ['note', 'resubmit', 'audit', 'comment', 'flag']):\n",
    "            print(f\"  -> [METADATA FOUND in '{key}']: {value}\")\n",
    "            found_metadata = True\n",
    "            \n",
    "    if not found_metadata:\n",
    "        print(\"  -> No audit notes or system flags detected.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3bd0ad",
   "metadata": {},
   "source": [
    "The global metadata inspection confirms that two records contain explicit audit flags: **app_042_duplicate** (Joseph Lopez) is marked as a \"**RESUBMISSION**\" and **app_001_duplicate** (Stephanie Nguyen) as a \"**DUPLICATE_ENTRY_ERROR**\". These notes provide evidence of **system integration failures** for these specific cases. However, the remaining **nine records**, including the **identity collisions** (Martinez/Wilson and Smith/Hill) and **missing SSNs** (Williams, Martin, Moore), lack any explanatory metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87d85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Uniqueness Health Metric ---\n",
      "Total Records: 502\n",
      "Records with Violations: 11\n",
      "Uniqueness Score: 97.81%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Uniqueness KPI to quantify data quality health\n",
    "total_records = collection.count_documents({})\n",
    "violation_count = sum(item['count'] for item in duplicates)\n",
    "unique_score = ((total_records - violation_count) / total_records) * 100\n",
    "\n",
    "# Output the Uniqueness Health Metric for the audit report\n",
    "print(f\"--- Uniqueness Health Metric ---\")\n",
    "print(f\"Total Records: {total_records}\")\n",
    "print(f\"Records with Violations: {violation_count}\")\n",
    "print(f\"Uniqueness Score: {unique_score:.2f}%\\n\")\n",
    "\n",
    "# Execute the remediation to quarantine failing records as required for deduplication\n",
    "# Map the specific IDs from the aggregation results into a flat list\n",
    "failing_ids = []\n",
    "for result in duplicates:\n",
    "    # Note: Ensure your pipeline above includes \"record_ids\": {\"$push\": \"$_id\"} to support this\n",
    "    failing_ids.extend(result.get('record_ids', []))\n",
    "\n",
    "# Move the identified violations to a dedicated quarantine collection\n",
    "if failing_ids:\n",
    "    quarantine_collection = db['quarantine_uniqueness']\n",
    "    \n",
    "    # Retrieve the full documents from the main collection\n",
    "    records_to_quarantine = list(collection.find({\"_id\": {\"$in\": failing_ids}}))\n",
    "    \n",
    "    # Insert the records into the quarantine collection and remove them from the active set\n",
    "    quarantine_collection.insert_many(records_to_quarantine)\n",
    "    collection.delete_many({\"_id\": {\"$in\": failing_ids}})\n",
    "    \n",
    "    print(f\"--- Remediation Executed ---\")\n",
    "    print(f\"Successfully moved {len(failing_ids)} records to 'quarantine_uniqueness'.\")\n",
    "    print(f\"Cleaned Baseline for further auditing: {collection.count_documents({})} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10692ba",
   "metadata": {},
   "source": [
    "**Uniqueness KPI and Remediation**\n",
    "\n",
    "To quantify the impact on data quality, a Uniqueness Health Metric of 97.81% is established, representing the ratio of compliant records to the total audit baseline. From a governance perspective, these quality gaps violate AI Act \"High-Quality Data\" standards (Art. 10) and compromise the \"Right to Explanation\" for affected individuals. Following the mandate to investigate and deduplicate, all 11 records are moved to the quarantine_uniqueness collection. This remediation ensures a clean, unique baseline of 491 documents for subsequent bias and fairness audits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c340327a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
